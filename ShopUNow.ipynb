{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMlBXe0FX0YHZCOKYT9k7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-disha/ShopUNow-Agent/blob/main/ShopUNow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#########################################################################################\n"
      ],
      "metadata": {
        "id": "iEqj7kzsa8Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tech Stack\n",
        "\n",
        "* LangChain / langchain_community-\tProvides VectorStores (FAISS), Document abstraction, Embeddings, and Retrieval.\n",
        "\n",
        "\n",
        "* FAISS (vectorstore)-\tFor embedding storage & similarity search (RAG).\n",
        "Sentence-Transformers embeddings-\tTo convert document chunks into embedding vectors.\n",
        "\n",
        "\n",
        "* **pdfminer.six + pytesseract + PIL**-\tExtract text from PDFs, images (OCR) and markdown/text files â€” for building corpus.\n",
        "\n",
        "\n",
        "* Markdownify\tConvert markdown files to plain text.\n",
        "\n",
        "\n",
        "* LangGraph (StateGraph etc.)-\tThe agent orchestration framework: state + nodes + transitions.\n",
        "\n",
        "\n",
        "* Pydantic-\tFor structured schemas of state and tool inputs (validation, typing).\n",
        "\n",
        "\n",
        "* LLM backends- OpenAI, Gemini (if available)\tFor synthesis / general LLM responses."
      ],
      "metadata": {
        "id": "lcNx5EjDfTQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parses different document types (text, csv, pdf, image) into a corpus.**\n",
        "\n",
        "**Chunks documents into manageable pieces using RecursiveCharacterTextSplitter.**\n",
        "\n",
        "**Builds a FAISS index, persists it.**\n",
        "\n",
        "**Sets up intent routing + tools for order status, returns, tickets.**\n",
        "\n",
        "**Handles RAG retrieval + LLM synthesis with system prompt.**\n",
        "\n",
        "**Passes retriever via RunnableConfig/configurable, avoiding earlier bug.**\n",
        "\n",
        "**Good structure using StateGraph, Pydantic state schemas.**"
      ],
      "metadata": {
        "id": "e27iQa2Rezzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell A â€“ Setup & LLM with safer secret handling (Render-safe)\n",
        "# =========================\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import threading\n",
        "\n",
        "# Detect runtime\n",
        "IS_RENDER = bool(os.getenv(\"PORT\") or os.getenv(\"RENDER_SERVICE_TYPE\"))\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Dev-only dependency install (skip in Render)\n",
        "# ---------------------------------------------\n",
        "if not IS_RENDER:\n",
        "    deps = [\n",
        "        \"langchain_community\", \"faiss-cpu\", \"langchain-openai\",\n",
        "        \"langchain-google-genai\", \"pydantic\", \"typing_extensions\",\n",
        "        \"vaderSentiment\", \"langgraph\", \"rapidfuzz\", \"flask\",\n",
        "        \"flask-cors\", \"pyngrok\"\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", *deps], check=False)\n",
        "    except Exception as ex:\n",
        "        print(\"âš ï¸ Dev install failed:\", ex, flush=True)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Secrets / API Keys\n",
        "# ---------------------------------------------\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not IS_RENDER:\n",
        "    try:\n",
        "        # Only attempt in Colab\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        OPENAI_API_KEY = OPENAI_API_KEY or userdata.get(\"OPENAI_API_KEY\")\n",
        "        GEMINI_API_KEY = GEMINI_API_KEY or userdata.get(\"GEMINI_API_KEY\")\n",
        "        if OPENAI_API_KEY:\n",
        "            os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "        if GEMINI_API_KEY:\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "    except Exception:\n",
        "        pass  # ignore if not in Colab\n",
        "\n",
        "if IS_RENDER:\n",
        "    assert OPENAI_API_KEY or GEMINI_API_KEY, (\n",
        "        \"Production requires OPENAI_API_KEY or GOOGLE_API_KEY.\"\n",
        "    )\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Lazy Initialization Flags\n",
        "# ---------------------------------------------\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
        "LLM = None\n",
        "vector_store = None\n",
        "faq_docs = None\n",
        "_initialized = False\n",
        "_init_lock = threading.Lock()\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Lazy Init Function (defers FAISS & LLM setup)\n",
        "# ---------------------------------------------\n",
        "def initialize_backend():\n",
        "    \"\"\"Thread-safe one-time heavy setup for embeddings, FAISS, and LLM.\"\"\"\n",
        "    global _initialized, vector_store, faq_docs, LLM\n",
        "    if _initialized:\n",
        "        return True\n",
        "\n",
        "    with _init_lock:\n",
        "        if _initialized:\n",
        "            return True\n",
        "        try:\n",
        "            print(\"âš™ï¸ Initializing FAISS + LLM â€¦\", flush=True)\n",
        "\n",
        "            # ---- Import heavy modules lazily ----\n",
        "            from langchain_openai import OpenAIEmbeddings\n",
        "            from langchain_core.documents import Document\n",
        "            from langchain_community.vectorstores import FAISS\n",
        "            from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "            import faiss\n",
        "\n",
        "            # ---- Tiny seed docs (safe for cold start) ----\n",
        "            faq_docs = [\n",
        "                Document(page_content=\"Support hours are 9 AMâ€“9 PM IST, Mondayâ€“Saturday\", metadata={\"department\": \"Customer Support\"}),\n",
        "                Document(page_content=\"Return window is 10 days from delivery\", metadata={\"department\": \"Orders & Returns\"}),\n",
        "                Document(page_content=\"We accept UPI, credit cards, wallets & COD\", metadata={\"department\": \"Payments & Billing\"})\n",
        "            ]\n",
        "\n",
        "            # ---- Embeddings + FAISS store ----\n",
        "            embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
        "            dim = len(embeddings.embed_query(\"hello world\"))\n",
        "            index = faiss.IndexFlatL2(dim)\n",
        "            vector_store = FAISS(\n",
        "                embedding_function=embeddings,\n",
        "                index=index,\n",
        "                docstore=InMemoryDocstore({}),\n",
        "                index_to_docstore_id={}\n",
        "            )\n",
        "            vector_store.add_documents(faq_docs, ids=[f\"doc{i}\" for i in range(len(faq_docs))])\n",
        "\n",
        "            # ---- Initialize LLM lazily ----\n",
        "            LLM = get_chat_model()\n",
        "\n",
        "            _initialized = True\n",
        "            print(\"âœ… Backend initialization complete.\", flush=True)\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"âŒ Backend init failed:\", e, flush=True)\n",
        "            return False\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Chat Model Getter\n",
        "# ---------------------------------------------\n",
        "def get_chat_model():\n",
        "    \"\"\"Returns available LLM (Gemini > OpenAI > mock).\"\"\"\n",
        "    if GEMINI_API_KEY:\n",
        "        try:\n",
        "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "            return ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
        "        except Exception as e:\n",
        "            print(\"Gemini init failed:\", e, flush=True)\n",
        "\n",
        "    if OPENAI_API_KEY:\n",
        "        try:\n",
        "            from langchain_openai import ChatOpenAI\n",
        "            return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "        except Exception as e:\n",
        "            print(\"OpenAI init failed:\", e, flush=True)\n",
        "\n",
        "    # Fallback mock model\n",
        "    from langchain_core.messages import HumanMessage\n",
        "    class _Mock:\n",
        "        def invoke(self, msgs):\n",
        "            last = next((m.content for m in reversed(msgs) if isinstance(m, HumanMessage)), \"\")\n",
        "            return type(\"Resp\", (), {\"content\": \"[MOCK] \" + last})\n",
        "    print(\"ğŸ§© Using mock LLM fallback\", flush=True)\n",
        "    return _Mock()\n",
        "\n",
        "SYSTEM_POLICY = (\n",
        "    \"You are ShopUNow Assistant. Be concise and accurate. \"\n",
        "    \"Use internal knowledge base when possible. \"\n",
        "    \"If unsure, ask a clarifying question.\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Cell A loaded â€” heavy init deferred until first use.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIkC2RVqCKeo",
        "outputId": "53222f4c-5c3d-4086-f423-fa3fd0f333d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cell A loaded â€” heavy init deferred until first use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell A.1 â€“ Lazy Load FAQ Dataset & Build Vector Store (Render-safe)\n",
        "# =========================\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import threading\n",
        "from typing import List, Tuple\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# --- Global cache + lock ---\n",
        "_FAQ_VECTOR_STORE = None\n",
        "_FAQ_DOCS: List[Document] = []\n",
        "_FAQ_PATH = None\n",
        "_FAQ_INIT_LOCK = threading.Lock()\n",
        "\n",
        "\n",
        "def resolve_faq_path() -> str:\n",
        "    \"\"\"\n",
        "    Resolve path to the FAQ JSONL file across common environments.\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        \"/content/shopunow_faqs.jsonl\",\n",
        "        os.path.join(os.getcwd(), \"data\", \"shopunow_faqs.jsonl\"),\n",
        "        os.path.join(os.getcwd(), \"shopunow_faqs.jsonl\"),\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        if os.path.exists(c):\n",
        "            return c\n",
        "    raise FileNotFoundError(\"âŒ shopunow_faqs.jsonl not found in common paths.\")\n",
        "\n",
        "\n",
        "def load_faq_documents(path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load and parse JSONL file into LangChain Documents.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                question = record.get(\"question\", \"\").strip()\n",
        "                answer = record.get(\"answer\", \"\").strip()\n",
        "                dept = record.get(\"department\", \"unknown\").strip()\n",
        "                if not question or not answer:\n",
        "                    continue\n",
        "                combined_text = f\"Q: {question}\\nA: {answer}\"\n",
        "                docs.append(\n",
        "                    Document(\n",
        "                        page_content=combined_text,\n",
        "                        metadata={\n",
        "                            \"department\": dept,\n",
        "                            \"question\": question,\n",
        "                            \"answer\": answer,\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"âš ï¸ Skipping invalid JSON line: {e}\")\n",
        "    return docs\n",
        "\n",
        "\n",
        "def build_faq_vector_store(docs: List[Document]) -> FAISS:\n",
        "    \"\"\"\n",
        "    Build FAISS vector store with normalized embeddings.\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        raise ValueError(\"No FAQ documents to build vector store.\")\n",
        "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "    dim = len(embedding_model.embed_query(\"hello world\"))\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    store = FAISS(\n",
        "        embedding_function=embedding_model,\n",
        "        index=index,\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "    )\n",
        "    ids = [f\"faq_{i+1}\" for i in range(len(docs))]\n",
        "    store.add_documents(docs, ids=ids)\n",
        "    return store\n",
        "\n",
        "\n",
        "def get_faq_vector_store() -> Tuple[FAISS, List[Document]]:\n",
        "    \"\"\"\n",
        "    Lazy initializer for the FAQ vector store.\n",
        "    Safe for multi-threaded Gunicorn startup.\n",
        "    \"\"\"\n",
        "    global _FAQ_VECTOR_STORE, _FAQ_DOCS, _FAQ_PATH\n",
        "\n",
        "    if _FAQ_VECTOR_STORE is not None:\n",
        "        return _FAQ_VECTOR_STORE, _FAQ_DOCS\n",
        "\n",
        "    with _FAQ_INIT_LOCK:\n",
        "        if _FAQ_VECTOR_STORE is not None:\n",
        "            return _FAQ_VECTOR_STORE, _FAQ_DOCS\n",
        "        try:\n",
        "            _FAQ_PATH = resolve_faq_path()\n",
        "            _FAQ_DOCS = load_faq_documents(_FAQ_PATH)\n",
        "            _FAQ_VECTOR_STORE = build_faq_vector_store(_FAQ_DOCS)\n",
        "            dept_set = {d.metadata.get(\"department\", \"unknown\") for d in _FAQ_DOCS}\n",
        "            print(\n",
        "                f\"âœ… Built vector store with {len(_FAQ_DOCS)} FAQs across {len(dept_set)} departments\",\n",
        "                flush=True,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error initializing FAQ store: {e}\", flush=True)\n",
        "            raise\n",
        "\n",
        "    return _FAQ_VECTOR_STORE, _FAQ_DOCS\n"
      ],
      "metadata": {
        "id": "n48dlld5MHiC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell B â€” Agent (lazy FAQ store, cosine/IP thresholds, escalation)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import Optional, List, Dict, Any, Literal, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "# LangChain base\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# âœ… Import lazy FAQ loader (refactored Cell A.1)\n",
        "try:\n",
        "    from shopunow_faq import get_faq_vector_store  # if using separate module\n",
        "except ImportError:\n",
        "    from __main__ import get_faq_vector_store      # fallback for Colab single-file mode\n",
        "\n",
        "# --------------------\n",
        "# Determinism\n",
        "# --------------------\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# --------------------\n",
        "# Lazy sentiment\n",
        "# --------------------\n",
        "_SENTIMENT = None\n",
        "def detect_sentiment(text: str) -> Literal[\"negative\", \"neutral\", \"positive\"]:\n",
        "    global _SENTIMENT\n",
        "    if _SENTIMENT is None:\n",
        "        _SENTIMENT = SentimentIntensityAnalyzer()\n",
        "    if not text:\n",
        "        return \"neutral\"\n",
        "    c = _SENTIMENT.polarity_scores(text).get(\"compound\", 0.0)\n",
        "    if c <= -0.3:\n",
        "        return \"negative\"\n",
        "    if c >= 0.3:\n",
        "        return \"positive\"\n",
        "    return \"neutral\"\n",
        "\n",
        "# --------------------\n",
        "# Department classifier\n",
        "# --------------------\n",
        "DEPT_KEYWORDS: Dict[str, List[str]] = {\n",
        "    \"Orders & Returns\": [\n",
        "        \"order\", \"order status\", \"track order\", \"tracking\", \"shipment\", \"delivery\",\n",
        "        \"package\", \"where is my order\", \"cancel order\", \"return\", \"refund\",\n",
        "        \"replace\", \"exchange\", \"pickup\"\n",
        "    ],\n",
        "    \"Payments & Billing\": [\n",
        "        \"payment\", \"upi\", \"card\", \"wallet\", \"cod\", \"invoice\", \"coupon\", \"billing\",\n",
        "        \"charged\", \"charge\", \"emi\", \"price\", \"gst\"\n",
        "    ],\n",
        "    \"Customer Support\": [\n",
        "        \"support\", \"contact\", \"help\", \"issue\", \"complaint\", \"agent\",\n",
        "        \"human\", \"speak to\", \"phone\", \"call\", \"email\", \"hours\", \"timings\"\n",
        "    ],\n",
        "    \"HR & IT Helpdesk\": [\n",
        "        \"password\", \"vpn\", \"access\", \"onboarding\", \"hardware\", \"software\",\n",
        "        \"leave\", \"policy\", \"salary\", \"payroll\", \"payslip\", \"stipend\",\n",
        "        \"salary date\", \"pay date\", \"salary delayed\", \"hrms\", \"hr portal\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "def classify_department_with_confidence(user_query: str) -> Tuple[Optional[str], float, Dict[str, int]]:\n",
        "    text = (user_query or \"\").lower()\n",
        "    scores = {dept: sum(1 for kw in kws if kw in text) for dept, kws in DEPT_KEYWORDS.items()}\n",
        "    top_dept, top_score = max(scores.items(), key=lambda x: x[1])\n",
        "    if top_score == 0:\n",
        "        return None, 0.0, scores\n",
        "    # Ambiguity guard\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    if len(sorted_scores) > 1 and sorted_scores[1][1] == top_score:\n",
        "        return None, 0.0, scores\n",
        "    conf = 0.6 if top_score == 1 else (0.75 if top_score == 2 else 0.9)\n",
        "    return top_dept, conf, scores\n",
        "\n",
        "# --------------------\n",
        "# Similarity thresholds\n",
        "# --------------------\n",
        "DEPT_SIM_THRESHOLDS = {\n",
        "    \"Orders & Returns\": 0.80,\n",
        "    \"Payments & Billing\": 0.78,\n",
        "    \"Customer Support\": 0.75,\n",
        "    \"HR & IT Helpdesk\": 0.80,\n",
        "    None: 0.80,\n",
        "}\n",
        "\n",
        "# --------------------\n",
        "# Agent State\n",
        "# --------------------\n",
        "class AgentState(BaseModel):\n",
        "    user_input: str\n",
        "    department: Optional[str] = None\n",
        "    dept_confidence: float = 0.0\n",
        "    sentiment: Optional[Literal[\"negative\",\"neutral\",\"positive\"]] = None\n",
        "    tools_used: List[str] = Field(default_factory=list)\n",
        "    retrieved: List[Dict[str, Any]] = Field(default_factory=list)\n",
        "    intent: Optional[Literal[\"rag\",\"order_status\",\"return_create\",\"ticket\",\"human_escalation\",\"unknown\"]] = None\n",
        "    answer: Optional[str] = None\n",
        "    confidence: float = 0.0\n",
        "    reason: Optional[str] = None\n",
        "\n",
        "# --------------------\n",
        "# Helpers\n",
        "# --------------------\n",
        "def extract_answer_text(page_content: str) -> str:\n",
        "    if not page_content:\n",
        "        return page_content\n",
        "    lc = page_content.lower()\n",
        "    if \"a:\" in lc:\n",
        "        idx = lc.find(\"a:\")\n",
        "        return page_content[idx + 2:].strip().lstrip(\":\").strip()\n",
        "    if page_content.strip().startswith(\"Q:\"):\n",
        "        return page_content.replace(\"Q:\", \"\", 1).strip()\n",
        "    return page_content\n",
        "\n",
        "def contains_any(text: str, keywords: List[str]) -> bool:\n",
        "    low = text.lower()\n",
        "    return any(kw in low for kw in keywords)\n",
        "\n",
        "# --------------------\n",
        "# Intent routing\n",
        "# --------------------\n",
        "def route_intent(state: AgentState) -> Dict[str, Any]:\n",
        "    q = (state.user_input or \"\").lower()\n",
        "    sentiment = detect_sentiment(state.user_input)\n",
        "    dept, conf, _ = classify_department_with_confidence(state.user_input)\n",
        "    if sentiment == \"negative\":\n",
        "        intent = \"human_escalation\"\n",
        "    elif contains_any(q, [\"order status\", \"track order\", \"where is my order\", \"tracking\", \"shipment\", \"delivery\", \"package\"]):\n",
        "        intent = \"order_status\"\n",
        "    elif contains_any(q, [\"return\", \"refund\", \"replace\", \"exchange\"]):\n",
        "        intent = \"rag\" if contains_any(q, [\"policy\", \"how many\", \"days\", \"window\"]) else \"return_create\"\n",
        "    elif contains_any(q, [\"ticket\", \"helpdesk\", \"support issue\", \"complaint\", \"problem\"]):\n",
        "        intent = \"ticket\"\n",
        "    else:\n",
        "        intent = \"rag\"\n",
        "    print(f\"[route_intent] â†’ intent={intent}, dept={dept}, conf={conf:.2f}, sentiment={sentiment}\", flush=True)\n",
        "    return {\"intent\": intent, \"department\": dept, \"dept_confidence\": conf, \"sentiment\": sentiment}\n",
        "\n",
        "# --------------------\n",
        "# Tool Node (lazy FAISS load)\n",
        "# --------------------\n",
        "def _filter_by_department(results, dept):\n",
        "    if not results or not dept:\n",
        "        return results or []\n",
        "    filtered = [(d, s) for d, s in results if (d.metadata or {}).get(\"department\") == dept]\n",
        "    return filtered or results\n",
        "\n",
        "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
        "    q = (state.user_input or \"\").strip()\n",
        "    dept, conf, intent = state.department, state.dept_confidence, state.intent\n",
        "    print(f\"[tool_node] intent={intent}, dept={dept}, conf={conf:.2f}\", flush=True)\n",
        "\n",
        "    # direct intents\n",
        "    if intent == \"order_status\":\n",
        "        has_id = any(tok.startswith((\"ORD-\", \"ord-\")) or tok.isdigit() for tok in q.replace(\"#\", \" \").split())\n",
        "        if not has_id:\n",
        "            return {\"answer\": \"Please share your Order ID (e.g. ORD-1234).\", \"tools_used\": [\"order_status_tool\"], \"confidence\": 0.6}\n",
        "        return {\"answer\": \"Your order is being processed and will be shipped soon.\", \"tools_used\": [\"order_status_tool\"], \"confidence\": 0.9}\n",
        "\n",
        "    if intent == \"return_create\":\n",
        "        return {\"answer\": \"Return initiated. Youâ€™ll get pickup and label details by email.\", \"tools_used\": [\"return_create_tool\"], \"confidence\": 0.9}\n",
        "\n",
        "    if intent == \"ticket\":\n",
        "        return {\"answer\": \"A support ticket has been created. Our team will reach out shortly.\", \"tools_used\": [\"ticket_tool\"], \"confidence\": 0.85}\n",
        "\n",
        "    if intent == \"human_escalation\":\n",
        "        return {\"answer\": \"Iâ€™m sorry for the inconvenience. Escalating to human support.\", \"tools_used\": [\"escalation\"], \"confidence\": 0.2}\n",
        "\n",
        "    # rag retrieval\n",
        "    if intent == \"rag\":\n",
        "        if not dept or conf < 0.6:\n",
        "            return {\"answer\": \"This may relate to multiple areas. Escalating to human support.\", \"tools_used\": [\"escalation\"], \"confidence\": 0.2}\n",
        "\n",
        "        try:\n",
        "            faq_store, faq_docs = get_faq_vector_store()\n",
        "        except Exception as e:\n",
        "            print(f\"[tool_node] âŒ FAQ init failed: {e}\", flush=True)\n",
        "            return {\"answer\": \"Error initializing knowledge base.\", \"tools_used\": [\"escalation\"], \"confidence\": 0.0}\n",
        "\n",
        "        try:\n",
        "            results = faq_store.similarity_search_with_score(q, k=5)\n",
        "            results = _filter_by_department(results, dept)\n",
        "            if not results:\n",
        "                return {\"answer\": \"Sorry, I couldnâ€™t find info. Escalating.\", \"tools_used\": [\"escalation\"], \"confidence\": 0.2}\n",
        "            doc, sim = results[0]\n",
        "            th = DEPT_SIM_THRESHOLDS.get(dept, 0.8)\n",
        "            if sim < th:\n",
        "                best_doc, best_fuzzy = None, 0.0\n",
        "                for d in faq_docs:\n",
        "                    fs = fuzz.partial_ratio(q.lower(), d.metadata.get(\"question\", \"\").lower()) / 100.0\n",
        "                    if fs > best_fuzzy:\n",
        "                        best_doc, best_fuzzy = d, fs\n",
        "                if best_doc and best_fuzzy >= 0.92:\n",
        "                    ans = extract_answer_text(best_doc.page_content)\n",
        "                    dept_meta = best_doc.metadata.get(\"department\", \"unknown\")\n",
        "                    return {\"answer\": f\"{ans} (Dept: {dept_meta})\", \"tools_used\": [\"rag_fuzzy_fallback\"], \"confidence\": float(best_fuzzy)}\n",
        "                return {\"answer\": \"Iâ€™m not confident in my answer. Escalating.\", \"tools_used\": [\"escalation\"], \"confidence\": float(sim)}\n",
        "            ans = extract_answer_text(doc.page_content)\n",
        "            dept_meta = doc.metadata.get(\"department\", \"unknown\")\n",
        "            return {\"answer\": f\"{ans} (Dept: {dept_meta})\", \"tools_used\": [\"rag_retrieval\"], \"confidence\": float(sim)}\n",
        "        except Exception as e:\n",
        "            print(f\"[tool_node] âŒ Retrieval failed: {e}\", flush=True)\n",
        "            return {\"answer\": \"Search error â€” escalating.\", \"tools_used\": [\"escalation\"], \"confidence\": 0.0}\n",
        "\n",
        "    # fallback\n",
        "    return {\"answer\": \"Could you please rephrase your question?\", \"tools_used\": [\"fallback\"], \"confidence\": 0.3}\n",
        "\n",
        "# --------------------\n",
        "# Graph\n",
        "# --------------------\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"route\", route_intent)\n",
        "graph.add_node(\"tool\", tool_node)\n",
        "graph.add_node(\"synth\", lambda s: {})\n",
        "graph.add_edge(START, \"route\")\n",
        "graph.add_edge(\"route\", \"tool\")\n",
        "graph.add_edge(\"tool\", \"synth\")\n",
        "graph.add_edge(\"synth\", END)\n",
        "\n",
        "app = graph.compile(checkpointer=MemorySaver())\n",
        "\n",
        "# --------------------\n",
        "# ask()\n",
        "# --------------------\n",
        "def ask(user_query: str, thread_id: Optional[str] = None) -> str:\n",
        "    import uuid\n",
        "    if not thread_id:\n",
        "        thread_id = f\"thread_{uuid.uuid4().hex}\"\n",
        "    out = app.invoke({\"user_input\": user_query}, config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "    return out.get(\"answer\", \"No answer generated.\")\n"
      ],
      "metadata": {
        "id": "18bhUnZECRuH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell B.1 - Testing (Improved Output & Meaningful Names)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "from pprint import pprint\n",
        "\n",
        "# Guard: only run this in dev / notebook environments, not in production (Render)\n",
        "if os.getenv(\"RENDER_SERVICE_TYPE\") or os.getenv(\"PORT\"):\n",
        "    print(\"[Skip] Testing cell running in production environment.\")\n",
        "else:\n",
        "    test_queries = [\n",
        "        \"What are your support hours?\",\n",
        "        \"Tell me order status for order id ORD-1234\",\n",
        "        \"I want a return because the product is wrong\",\n",
        "        \"My password reset isn't working, this is frustrating\",\n",
        "        \"I submitted a complaint about a support issue\",\n",
        "        \"How do I pay with UPI?\",\n",
        "        \"How to apply for leaves?\",\n",
        "        \"what is the leave policy?\",\n",
        "        \"where is my order?\",\n",
        "        \"my order is delayed\",\n",
        "        \"whats the return polcy? how many days can i return the product?\",\n",
        "        \"how many leaves can I take?\",\n",
        "        \" how many days for rturn?\",\n",
        "        \"my retturn is delayed\",\n",
        "        \"Please replace my shirt size\",\n",
        "        \"where is my order\",\n",
        "        \"when will i get my salary\",\n",
        "        \"my salaray is delayed\",\n",
        "        \"I need help,\",  # ambiguous\n",
        "    ]\n",
        "\n",
        "    for user_query in test_queries:\n",
        "        conversation_id = f\"conv_{uuid.uuid4().hex}\"\n",
        "        try:\n",
        "            state = AgentState(user_input=user_query)\n",
        "            route_info = route_intent(state)\n",
        "            response = app.invoke(\n",
        "                {\"user_input\": user_query},\n",
        "                config={\"configurable\": {\"thread_id\": conversation_id}}\n",
        "            )\n",
        "\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"ğŸ§‘ User Query: {user_query}\")\n",
        "            print(f\"â¡ï¸ Intent: {route_info.get('intent')} | Dept: {route_info.get('department')} | Sentiment: {route_info.get('sentiment')}\")\n",
        "            print(f\"ğŸ¤– Agent Answer: {response.get('answer', 'âš ï¸ No answer')}\")\n",
        "            print(f\"ğŸ› ï¸ Tools Used: {response.get('tools_used')}\")\n",
        "            if response.get(\"retrieved\"):\n",
        "                print(\"ğŸ“„ Retrieved Context:\")\n",
        "                pprint(response[\"retrieved\"])\n",
        "            print(\"=\" * 80 + \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error handling test query {user_query!r}: {e}\")\n",
        "            traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL6WcVgTCuGf",
        "outputId": "d40f35d4-82b2-4389-fe3e-d56165df57a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[route_intent] â†’ intent=rag, dept=Customer Support, conf=0.75, sentiment=positive\n",
            "[route_intent] â†’ intent=rag, dept=Customer Support, conf=0.75, sentiment=positive\n",
            "[tool_node] intent=rag, dept=Customer Support, conf=0.75\n",
            "âœ… Built vector store with 119 FAQs across 5 departments\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: What are your support hours?\n",
            "â¡ï¸ Intent: rag | Dept: Customer Support | Sentiment: positive\n",
            "ğŸ¤– Agent Answer: Yes, live chat is available from 9 AM to 9 PM IST on our website and mobile app. (Dept: Customer Support)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=order_status, dept=Orders & Returns, conf=0.75, sentiment=neutral\n",
            "[route_intent] â†’ intent=order_status, dept=Orders & Returns, conf=0.75, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, conf=0.75\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: Tell me order status for order id ORD-1234\n",
            "â¡ï¸ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: Your order is being processed and will be shipped soon.\n",
            "ğŸ› ï¸ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=human_escalation, dept=Orders & Returns, conf=0.60, sentiment=negative\n",
            "[route_intent] â†’ intent=human_escalation, dept=Orders & Returns, conf=0.60, sentiment=negative\n",
            "[tool_node] intent=human_escalation, dept=Orders & Returns, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: I want a return because the product is wrong\n",
            "â¡ï¸ Intent: human_escalation | Dept: Orders & Returns | Sentiment: negative\n",
            "ğŸ¤– Agent Answer: Iâ€™m sorry for the inconvenience. Escalating to human support.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=human_escalation, dept=HR & IT Helpdesk, conf=0.60, sentiment=negative\n",
            "[route_intent] â†’ intent=human_escalation, dept=HR & IT Helpdesk, conf=0.60, sentiment=negative\n",
            "[tool_node] intent=human_escalation, dept=HR & IT Helpdesk, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: My password reset isn't working, this is frustrating\n",
            "â¡ï¸ Intent: human_escalation | Dept: HR & IT Helpdesk | Sentiment: negative\n",
            "ğŸ¤– Agent Answer: Iâ€™m sorry for the inconvenience. Escalating to human support.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=ticket, dept=Customer Support, conf=0.90, sentiment=neutral\n",
            "[route_intent] â†’ intent=ticket, dept=Customer Support, conf=0.90, sentiment=neutral\n",
            "[tool_node] intent=ticket, dept=Customer Support, conf=0.90\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: I submitted a complaint about a support issue\n",
            "â¡ï¸ Intent: ticket | Dept: Customer Support | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: A support ticket has been created. Our team will reach out shortly.\n",
            "ğŸ› ï¸ Tools Used: ['ticket_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=Payments & Billing, conf=0.60, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=Payments & Billing, conf=0.60, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=Payments & Billing, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: How do I pay with UPI?\n",
            "â¡ï¸ Intent: rag | Dept: Payments & Billing | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: At checkout, select UPI, click 'Add New ID', and complete verification. (Dept: Payments & Billing)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.60, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.60, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: How to apply for leaves?\n",
            "â¡ï¸ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: You can apply for leave via the HRMS portal with manager approval. (Dept: HR & IT Helpdesk)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.75, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.75, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, conf=0.75\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: what is the leave policy?\n",
            "â¡ï¸ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: Employees are entitled to 20 days of annual leave, with manager approval required. (Dept: HR & IT Helpdesk)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=order_status, dept=Orders & Returns, conf=0.75, sentiment=neutral\n",
            "[route_intent] â†’ intent=order_status, dept=Orders & Returns, conf=0.75, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, conf=0.75\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: where is my order?\n",
            "â¡ï¸ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: Please share your Order ID (e.g. ORD-1234).\n",
            "ğŸ› ï¸ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=Orders & Returns, conf=0.60, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=Orders & Returns, conf=0.60, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=Orders & Returns, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: my order is delayed\n",
            "â¡ï¸ Intent: rag | Dept: Orders & Returns | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: If your order is delayed, please check your tracking page or contact support for assistance. (Dept: Orders & Returns)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=Orders & Returns, conf=0.60, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=Orders & Returns, conf=0.60, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=Orders & Returns, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: whats the return polcy? how many days can i return the product?\n",
            "â¡ï¸ Intent: rag | Dept: Orders & Returns | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: You can return items within 10 days of delivery for a full refund. (Dept: Orders & Returns)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.60, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.60, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: how many leaves can I take?\n",
            "â¡ï¸ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: Employees can take up to 20 days of annual leave per year. Carryover is subject to approval. (Dept: HR & IT Helpdesk)\n",
            "ğŸ› ï¸ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=None, conf=0.00, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=None, conf=0.00, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=None, conf=0.00\n",
            "================================================================================\n",
            "ğŸ§‘ User Query:  how many days for rturn?\n",
            "â¡ï¸ Intent: rag | Dept: None | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: This may relate to multiple areas. Escalating to human support.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=None, conf=0.00, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=None, conf=0.00, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=None, conf=0.00\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: my retturn is delayed\n",
            "â¡ï¸ Intent: rag | Dept: None | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: This may relate to multiple areas. Escalating to human support.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=return_create, dept=Orders & Returns, conf=0.60, sentiment=positive\n",
            "[route_intent] â†’ intent=return_create, dept=Orders & Returns, conf=0.60, sentiment=positive\n",
            "[tool_node] intent=return_create, dept=Orders & Returns, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: Please replace my shirt size\n",
            "â¡ï¸ Intent: return_create | Dept: Orders & Returns | Sentiment: positive\n",
            "ğŸ¤– Agent Answer: Return initiated. Youâ€™ll get pickup and label details by email.\n",
            "ğŸ› ï¸ Tools Used: ['return_create_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=order_status, dept=Orders & Returns, conf=0.75, sentiment=neutral\n",
            "[route_intent] â†’ intent=order_status, dept=Orders & Returns, conf=0.75, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, conf=0.75\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: where is my order\n",
            "â¡ï¸ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: Please share your Order ID (e.g. ORD-1234).\n",
            "ğŸ› ï¸ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.60, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=HR & IT Helpdesk, conf=0.60, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: when will i get my salary\n",
            "â¡ï¸ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: Iâ€™m not confident in my answer. Escalating.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=None, conf=0.00, sentiment=neutral\n",
            "[route_intent] â†’ intent=rag, dept=None, conf=0.00, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=None, conf=0.00\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: my salaray is delayed\n",
            "â¡ï¸ Intent: rag | Dept: None | Sentiment: neutral\n",
            "ğŸ¤– Agent Answer: This may relate to multiple areas. Escalating to human support.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] â†’ intent=rag, dept=Customer Support, conf=0.60, sentiment=positive\n",
            "[route_intent] â†’ intent=rag, dept=Customer Support, conf=0.60, sentiment=positive\n",
            "[tool_node] intent=rag, dept=Customer Support, conf=0.60\n",
            "================================================================================\n",
            "ğŸ§‘ User Query: I need help,\n",
            "â¡ï¸ Intent: rag | Dept: Customer Support | Sentiment: positive\n",
            "ğŸ¤– Agent Answer: Iâ€™m not confident in my answer. Escalating.\n",
            "ğŸ› ï¸ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell C â€” Flask API (Render + Colab Compatible, Fast Bind)\n",
        "# =========================\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import threading\n",
        "import uuid\n",
        "import socket\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "# --- Flask setup ---\n",
        "flask_app = Flask(__name__)\n",
        "CORS(flask_app)\n",
        "\n",
        "def _debug(msg: str):\n",
        "    \"\"\"Flush-safe logger.\"\"\"\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# --------------------\n",
        "# Agent bridge (lazy)\n",
        "# --------------------\n",
        "def call_agent(query: str) -> str:\n",
        "    \"\"\"Safely route query to any active agent.\"\"\"\n",
        "    try:\n",
        "        if \"ask\" in globals() and callable(globals()[\"ask\"]):\n",
        "            _debug(\"[AGENT] Using ask()\")\n",
        "            return globals()[\"ask\"](query)\n",
        "        for name in [\"agent_app\", \"graph_app\", \"app\"]:\n",
        "            obj = globals().get(name)\n",
        "            if hasattr(obj, \"invoke\"):\n",
        "                _debug(f\"[AGENT] Using {name}.invoke()\")\n",
        "                out = obj.invoke(\n",
        "                    {\"user_input\": query},\n",
        "                    config={\"configurable\": {\"thread_id\": f\"api-{uuid.uuid4().hex}\"}}\n",
        "                )\n",
        "                return out.get(\"answer\", \"No answer generated.\")\n",
        "        return \"âš ï¸ No active agent found.\"\n",
        "    except Exception as e:\n",
        "        _debug(f\"[AGENT] âŒ Error in call_agent: {e}\")\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "        return f\"Internal error: {e}\"\n",
        "\n",
        "# --------------------\n",
        "# Routes\n",
        "# --------------------\n",
        "@flask_app.route(\"/ask\", methods=[\"POST\", \"GET\"])\n",
        "def ask_api():\n",
        "    try:\n",
        "        _debug(\"[API] /ask hit\")\n",
        "        query = \"\"\n",
        "        if request.method == \"POST\":\n",
        "            data = request.get_json(silent=True) or {}\n",
        "            query = (data.get(\"query\") or \"\").strip()\n",
        "        else:\n",
        "            query = (request.args.get(\"query\") or \"\").strip()\n",
        "\n",
        "        # If no query in GET, show a simple greeting or instructions\n",
        "        if request.method == \"GET\" and not query:\n",
        "            return jsonify({\"message\": \"Welcome to ShopUNow API. Use /ask?query=hello or POST JSON {\\\"query\\\": ...}\"}), 200\n",
        "\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Empty query\"}), 400\n",
        "\n",
        "        _debug(f\"[API] Query: {query!r}\")\n",
        "        answer = call_agent(query)\n",
        "        _debug(f\"[API] Answer: {answer!r}\")\n",
        "        return jsonify({\"query\": query, \"answer\": answer})\n",
        "\n",
        "    except Exception as e:\n",
        "        _debug(\"[API] âŒ Exception:\")\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "        return jsonify({\"error\": \"Internal Server Error\", \"details\": str(e)}), 500\n",
        "\n",
        "@flask_app.route(\"/\", methods=[\"GET\"])\n",
        "def home():\n",
        "    return jsonify({\"status\": \"ok\", \"message\": \"ShopUNow backend active\"})\n",
        "\n",
        "# --------------------\n",
        "# Colab / Render detection + ngrok (dev) support\n",
        "# --------------------\n",
        "def _get_colab_secret(name: str):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(name)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _find_free_port():\n",
        "    \"\"\"Find a free port on localhost.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind((\"\", 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "IS_RENDER = bool(os.getenv(\"RENDER_SERVICE_TYPE\") or os.getenv(\"PORT\"))\n",
        "NGROK_AUTH_TOKEN = _get_colab_secret(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Colab / Dev Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if not IS_RENDER and NGROK_AUTH_TOKEN:\n",
        "    PORT = _find_free_port()\n",
        "    _debug(f\"â–¶ï¸ [Colab Mode] Starting Flask on port {PORT}\")\n",
        "\n",
        "    def _run_flask():\n",
        "        flask_app.run(host=\"0.0.0.0\", port=PORT, debug=False, use_reloader=False)\n",
        "    threading.Thread(target=_run_flask, daemon=True).start()\n",
        "\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyngrok\"], check=False)\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "    try:\n",
        "        _debug(\"ğŸ”‘ Setting ngrok auth token\")\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        ngrok.kill()\n",
        "        tunnel = ngrok.connect(PORT, bind_tls=True)\n",
        "        public_url = getattr(tunnel, \"public_url\", str(tunnel))\n",
        "        _debug(f\"ğŸš€ Public URL: {public_url}\")\n",
        "\n",
        "        with open(\"/content/backend_url.txt\", \"w\") as f:\n",
        "            f.write(public_url.strip())\n",
        "        _debug(\"ğŸ“ backend_url.txt written\")\n",
        "\n",
        "        print(\"\\nâœ… Test using this:\")\n",
        "        print(f'curl -X POST \"{public_url}/ask\" -H \"Content-Type: application/json\" -d \\'{{\"query\":\"Hello\"}}\\'')\n",
        "    except Exception as e:\n",
        "        _debug(f\"âŒ ngrok setup failed: {e}\")\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Render / Production Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "else:\n",
        "    PORT = int(os.getenv(\"PORT\", 8000))\n",
        "    _debug(f\"â–¶ï¸ [Render Mode] Starting Flask on 0.0.0.0:{PORT}\")\n",
        "\n",
        "    # Gunicorn will manage the server, so we only call run if __main__\n",
        "    if __name__ == \"__main__\":\n",
        "        _debug(\"ğŸš€ Running Flask (dev fallback)\")\n",
        "        flask_app.run(host=\"0.0.0.0\", port=PORT, debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgBZx70SRRCQ",
        "outputId": "aa84c548-07d1-45df-97d3-482ca9e8c2a3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–¶ï¸ [Colab Mode] Starting Flask on port 52265\n",
            "ğŸ”‘ Setting ngrok auth token * Serving Flask app '__main__'\n",
            "\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:52265\n",
            " * Running on http://172.28.0.12:52265\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Public URL: https://eecc914de7a2.ngrok-free.app\n",
            "ğŸ“ backend_url.txt written\n",
            "\n",
            "âœ… Test using this:\n",
            "curl -X POST \"https://eecc914de7a2.ngrok-free.app/ask\" -H \"Content-Type: application/json\" -d '{\"query\":\"Hello\"}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell D â€” Streamlit Frontend (Colab + Render)\n",
        "# =========================\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "IS_RENDER = bool(os.getenv(\"PORT\") or os.getenv(\"RENDER_SERVICE_TYPE\"))\n",
        "\n",
        "def _debug(msg: str):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ”¹ COMMON STREAMLIT APP TEMPLATE\n",
        "# ============================================================\n",
        "def write_streamlit_app(default_api_url: str):\n",
        "    app_code = f\"\"\"\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "st.set_page_config(page_title=\"ShopUNow Agent\", layout=\"centered\")\n",
        "st.title(\"ğŸ›ï¸ ShopUNow AI Assistant\")\n",
        "\n",
        "api_url = st.sidebar.text_input(\"Backend URL\", value=\"{default_api_url}\")\n",
        "st.sidebar.caption(\"URL to Flask /ask endpoint\")\n",
        "\n",
        "st.divider()\n",
        "st.subheader(\"ğŸ’¬ Chat\")\n",
        "\n",
        "if \"chat\" not in st.session_state:\n",
        "    st.session_state.chat = []\n",
        "\n",
        "query = st.text_input(\"Ask something:\")\n",
        "\n",
        "if st.button(\"Ask\"):\n",
        "    if query.strip():\n",
        "        st.session_state.chat.append((\"ğŸ§‘ You\", query))\n",
        "        try:\n",
        "            resp = requests.post(api_url, json={{\"query\": query}}, timeout=25)\n",
        "            if resp.status_code == 200:\n",
        "                ans = resp.json().get(\"answer\", \"âš ï¸ No answer returned.\")\n",
        "            else:\n",
        "                ans = f\"âš ï¸ HTTP {{resp.status_code}}: {{resp.text}}\"\n",
        "        except Exception as e:\n",
        "            ans = f\"âš ï¸ Request failed: {{e}}\"\n",
        "        st.session_state.chat.append((\"ğŸ¤– Agent\", ans))\n",
        "\n",
        "for sender, msg in st.session_state.chat:\n",
        "    st.markdown(f\"**{{sender}}:** {{msg}}\")\n",
        "\"\"\"\n",
        "    with open(\"app_frontend.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(app_code.strip())\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ”¹ RENDER MODE (Production)\n",
        "# ============================================================\n",
        "if IS_RENDER:\n",
        "    _debug(\"[Render] Setting up Streamlit frontend\")\n",
        "\n",
        "    try:\n",
        "        import streamlit, requests\n",
        "    except ImportError:\n",
        "        deps = [\"streamlit\", \"requests\"]\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", *deps])\n",
        "        import streamlit, requests\n",
        "\n",
        "    # Prefer BACKEND_URL from environment; fallback to same host\n",
        "    backend_url = os.getenv(\"BACKEND_URL\", \"\").strip().rstrip(\"/\")\n",
        "    default_api_url = backend_url + \"/ask\" if backend_url else \"/ask\"\n",
        "\n",
        "    write_streamlit_app(default_api_url)\n",
        "\n",
        "    # On Render, gunicorn/startCommand will launch Streamlit:\n",
        "    # streamlit run app_frontend.py --server.port $PORT\n",
        "    # âœ… No background thread here â€” Render runs it automatically\n",
        "    _debug(f\"âœ… Streamlit app ready. BACKEND_URL={default_api_url}\")\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ”¹ COLAB / DEV MODE (Auto-ngrok)\n",
        "# ============================================================\n",
        "else:\n",
        "    _debug(\"[Colab/Dev] Launching Streamlit frontend...\")\n",
        "\n",
        "    try:\n",
        "        import streamlit, requests\n",
        "        from pyngrok import ngrok\n",
        "    except ImportError:\n",
        "        deps = [\"streamlit\", \"requests\", \"pyngrok\"]\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", *deps])\n",
        "        import streamlit, requests\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "    backend_url_file = \"/content/backend_url.txt\"\n",
        "    default_api_url = \"http://127.0.0.1:5000/ask\"\n",
        "\n",
        "    if os.path.exists(backend_url_file):\n",
        "        try:\n",
        "            with open(backend_url_file, \"r\") as f:\n",
        "                url = f.read().strip()\n",
        "            if url:\n",
        "                default_api_url = url.rstrip(\"/\") + \"/ask\"\n",
        "                _debug(f\"âœ… Backend URL loaded: {default_api_url}\")\n",
        "        except Exception as e:\n",
        "            _debug(f\"âš ï¸ Could not read backend_url.txt: {e}\")\n",
        "\n",
        "    write_streamlit_app(default_api_url)\n",
        "\n",
        "    # Launch Streamlit in background\n",
        "    def run_frontend():\n",
        "        _debug(\"â–¶ï¸ Starting Streamlit on port 8501...\")\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"streamlit\", \"run\", \"app_frontend.py\",\n",
        "            \"--server.port\", \"8501\", \"--server.headless\", \"true\"\n",
        "        ])\n",
        "\n",
        "    threading.Thread(target=run_frontend, daemon=True).start()\n",
        "    time.sleep(6)\n",
        "\n",
        "    try:\n",
        "        _debug(\"ğŸŒ Opening ngrok tunnel for Streamlit (8501)...\")\n",
        "        tunnel = ngrok.connect(8501, bind_tls=True)\n",
        "        public_url = getattr(tunnel, \"public_url\", str(tunnel))\n",
        "        _debug(f\"ğŸš€ Streamlit frontend public URL: {public_url}\")\n",
        "    except Exception as e:\n",
        "        _debug(f\"âš ï¸ ngrok tunnel failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgocOgNHW72d",
        "outputId": "050e1624-6adb-4842-83ae-0bd407dc600e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Colab/Dev] Launching Streamlit frontend...\n",
            "âœ… Backend URL loaded: https://eecc914de7a2.ngrok-free.app/ask\n",
            "â–¶ï¸ Starting Streamlit on port 8501...\n",
            "ğŸŒ Opening ngrok tunnel for Streamlit (8501)...\n",
            "ğŸš€ Streamlit frontend public URL: https://83523456fd41.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}