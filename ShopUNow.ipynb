{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOd+HF/hTLMa4Mf2c6Elq0J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-disha/ShopUNow-Agent/blob/main/ShopUNow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####################################\n"
      ],
      "metadata": {
        "id": "iEqj7kzsa8Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tech Stack\n",
        "\n",
        "LangChain / langchain_community-\tProvides VectorStores (FAISS), Document abstraction, Embeddings, and Retrieval.\n",
        "FAISS (vectorstore)-\tFor embedding storage & similarity search (RAG).\n",
        "Sentence-Transformers embeddings-\tTo convert document chunks into embedding vectors.\n",
        "**pdfminer.six + pytesseract + PIL**-\tExtract text from PDFs, images (OCR) and markdown/text files — for building corpus.\n",
        "Markdownify\tConvert markdown files to plain text.\n",
        "LangGraph (StateGraph etc.)-\tThe agent orchestration framework: state + nodes + transitions.\n",
        "Pydantic-\tFor structured schemas of state and tool inputs (validation, typing).\n",
        "LLM backends- OpenAI, Gemini (if available)\tFor synthesis / general LLM responses."
      ],
      "metadata": {
        "id": "lcNx5EjDfTQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parses different document types (text, csv, pdf, image) into a corpus.**\n",
        "\n",
        "**Chunks documents into manageable pieces using RecursiveCharacterTextSplitter.**\n",
        "\n",
        "**Builds a FAISS index, persists it.**\n",
        "\n",
        "**Sets up intent routing + tools for order status, returns, tickets.**\n",
        "\n",
        "**Handles RAG retrieval + LLM synthesis with system prompt.**\n",
        "\n",
        "**Passes retriever via RunnableConfig/configurable, avoiding earlier bug.**\n",
        "\n",
        "**Good structure using StateGraph, Pydantic state schemas.**"
      ],
      "metadata": {
        "id": "e27iQa2Rezzd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d82ad50"
      },
      "source": [
        "# Task\n",
        "Modify the code to use Gemini as the primary LLM and fallback to OpenAI, and add tools for order status, returns, and tickets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f6e900c"
      },
      "source": [
        "## Integrate llms (gemini and openai)\n",
        "\n",
        "### Subtask:\n",
        "Modify the agent to use a language model for answer synthesis, with Gemini as the primary and OpenAI as a fallback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091b1bad"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `get_chat_model` function to handle primary (Gemini) and fallback (OpenAI) LLM initialization and then update the `synthesis_node` to use this LLM for answer generation based on context and user input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7121c35"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `langchain_google_genai` was not installed. Install the missing package and try the imports and function definition again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell A: Setup and Vector Store + LLM\n",
        "# =========================\n",
        "\n",
        "# Install required packages (include Google-GenAI integration if using Gemini)\n",
        "!pip install -qU langchain_community faiss-cpu langchain_openai langchain-google-genai pydantic typing_extensions vaderSentiment langgraph\n",
        "\n",
        "!pip install -qU flask flask-cors pyngrok\n",
        "\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional, Literal\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Make sure API keys are set\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    if key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    gem_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "    if gem_key:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = gem_key\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "GEMINI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")  # for Gemini\n",
        "\n",
        "assert OPENAI_API_KEY or GEMINI_API_KEY, \"Please set OPENAI_API_KEY or GEMINI_API_KEY in environment variables.\"\n",
        "\n",
        "# Initialize embeddings (use OpenAI embeddings; you can use Gemini embeddings if you want and have the key)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Sample documents / FAQ data with department metadata\n",
        "# You should expand these to 10-15 QA per department later\n",
        "faq_docs = [\n",
        "    Document(page_content=\"Support hours are 9 AM–9 PM IST, Monday to Saturday\", metadata={\"department\": \"Customer Support\"}),\n",
        "    Document(page_content=\"How to contact support email or phone\", metadata={\"department\": \"Customer Support\"}),\n",
        "    Document(page_content=\"Return window is 10 days from delivery\", metadata={\"department\": \"Orders & Returns\"}),\n",
        "    Document(page_content=\"How can I initiate a return process\", metadata={\"department\": \"Orders & Returns\"}),\n",
        "    Document(page_content=\"We accept UPI, credit cards, wallets, and COD\", metadata={\"department\": \"Payments & Billing\"}),\n",
        "    Document(page_content=\"How to apply coupon at checkout\", metadata={\"department\": \"Payments & Billing\"})\n",
        "]\n",
        "\n",
        "# Build the FAISS vector store\n",
        "import faiss\n",
        "\n",
        "# Compute embedding dimension\n",
        "dim = len(embeddings.embed_query(\"hello world\"))  # length of vector\n",
        "\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore({}),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "ids = [f\"doc{i+1}\" for i in range(len(faq_docs))]\n",
        "vector_store.add_documents(documents=faq_docs, ids=ids)\n",
        "\n",
        "# Initialize LLM (Chat model)\n",
        "def get_chat_model():\n",
        "    if GEMINI_API_KEY:\n",
        "        try:\n",
        "            print(\"Using Gemini LLM\")\n",
        "            return ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
        "        except Exception as e:\n",
        "            print(\"Gemini init failed:\", e)\n",
        "    if OPENAI_API_KEY:\n",
        "        try:\n",
        "            print(\"Using OpenAI model\")\n",
        "            return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "        except Exception as e:\n",
        "            print(\"OpenAI init failed:\", e)\n",
        "    # fallback mock model\n",
        "    class _Mock:\n",
        "        def invoke(self, messages: List[Any]):\n",
        "            last_user = None\n",
        "            for m in reversed(messages):\n",
        "                if isinstance(m, HumanMessage):\n",
        "                    last_user = m\n",
        "                    break\n",
        "            return type(\"Obj\", (), {\"content\": \"[MOCK] \" + (last_user.content if last_user else \"\")})\n",
        "    print(\"Using mock LLM fallback\")\n",
        "    return _Mock()\n",
        "\n",
        "LLM = get_chat_model()\n",
        "\n",
        "# Optionally define system policy / prompt template\n",
        "SYSTEM_POLICY = (\n",
        "    \"You are ShopUNow Assistant. Be concise and accurate. Use the internal knowledge base when possible. \"\n",
        "    \"Cite sources. If you cannot answer, ask a clarifying question.\"\n",
        ")\n",
        "\n",
        "print(\"Cell A setup complete: vector_store and LLM are initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIkC2RVqCKeo",
        "outputId": "fa4ad078-3def-4c75-ed4a-e767e80cd78c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Gemini LLM\n",
            "Cell A setup complete: vector_store and LLM are initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell A.1 - Load FAQ Dataset & Build Vector Store\n",
        "# =========================\n",
        "import json\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "\n",
        "# ---- Step 1: Load JSONL file ----\n",
        "jsonl_path = \"/content/shopunow_faqs.jsonl\"\n",
        "docs = []\n",
        "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:   # skip empty lines\n",
        "            continue\n",
        "        try:\n",
        "            record = json.loads(line)\n",
        "            docs.append(\n",
        "                Document(\n",
        "                    page_content=record[\"answer\"],\n",
        "                    metadata={\n",
        "                        \"department\": record.get(\"department\", \"unknown\"),\n",
        "                        \"question\": record.get(\"question\", \"\")\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"⚠️ Skipping bad line: {line[:80]}... | Error: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(docs)} FAQs from {jsonl_path}\")\n",
        "\n",
        "# ---- Step 2: Build FAISS Vector Store ----\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "dim = len(embeddings.embed_query(\"hello world\"))\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "ids = [f\"doc{i+1}\" for i in range(len(docs))]\n",
        "vector_store.add_documents(documents=docs, ids=ids)\n",
        "\n",
        "print(f\"✅ Vector store built with {len(docs)} documents across {len(set(d.metadata['department'] for d in docs))} departments\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n48dlld5MHiC",
        "outputId": "69d285a4-dfcf-4cad-c91c-4a8e6271467c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 75 FAQs from /content/shopunow_faqs.jsonl\n",
            "✅ Vector store built with 75 documents across 5 departments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell B - Agent Definition with JSONL Vector Store\n",
        "# =========================\n",
        "import json\n",
        "import faiss\n",
        "from typing import Optional, List, Dict, Any, Literal\n",
        "from typing_extensions import Annotated\n",
        "from operator import add\n",
        "from pydantic import BaseModel, Field\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# ---- Load FAQ dataset from JSONL ----\n",
        "jsonl_path = \"/content/shopunow_faqs.jsonl\"\n",
        "docs = []\n",
        "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        record = json.loads(line)\n",
        "        docs.append(\n",
        "            Document(\n",
        "                page_content=record[\"answer\"],\n",
        "                metadata={\"department\": record[\"department\"], \"question\": record[\"question\"]}\n",
        "            )\n",
        "        )\n",
        "\n",
        "print(f\"Loaded {len(docs)} FAQs from {jsonl_path}\")\n",
        "\n",
        "# ---- Build FAISS vector store ----\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "dim = len(embeddings.embed_query(\"hello world\"))\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "ids = [f\"doc{i+1}\" for i in range(len(docs))]\n",
        "vector_store.add_documents(documents=docs, ids=ids)\n",
        "\n",
        "print(f\"✅ Vector store ready with {len(docs)} documents across {len(set(d.metadata['department'] for d in docs))} departments\")\n",
        "\n",
        "# ---- Sentiment + Dept Classifier ----\n",
        "_sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def detect_sentiment(text: str) -> Literal[\"negative\",\"neutral\",\"positive\"]:\n",
        "    if not text:\n",
        "        return \"neutral\"\n",
        "    scores = _sentiment_analyzer.polarity_scores(text)\n",
        "    c = scores.get(\"compound\", 0.0)\n",
        "    if c <= -0.3:\n",
        "        return \"negative\"\n",
        "    if c >= 0.3:\n",
        "        return \"positive\"\n",
        "    return \"neutral\"\n",
        "\n",
        "def classify_department(text: str) -> Optional[str]:\n",
        "    t = (text or \"\").lower()\n",
        "    if any(kw in t for kw in [\"order status\", \"track order\", \"where is my order\", \"order tracking\"]):\n",
        "        return \"Orders & Returns\"\n",
        "    if any(kw in t for kw in [\"return\", \"refund\", \"replace\", \"exchange\"]):\n",
        "        return \"Orders & Returns\"\n",
        "    if any(kw in t for kw in [\"payment\", \"upi\", \"card\", \"wallet\", \"cod\", \"invoice\", \"coupon\"]):\n",
        "        return \"Payments & Billing\"\n",
        "    if any(kw in t for kw in [\"support\", \"contact\", \"help\", \"issue\", \"complaint\"]):\n",
        "        return \"Customer Support\"\n",
        "    if any(kw in t for kw in [\"password\", \"vpn\", \"access\", \"onboarding\", \"hardware\", \"software\"]):\n",
        "        return \"HR & IT Helpdesk\"\n",
        "    return None\n",
        "\n",
        "# ---- Agent State ----\n",
        "class AgentState(BaseModel):\n",
        "    user_input: str\n",
        "    department: Optional[str] = None\n",
        "    sentiment: Optional[Literal[\"negative\",\"neutral\",\"positive\"]] = None\n",
        "    tools_used: Annotated[List[str], add] = Field(default_factory=list)\n",
        "    retrieved: Annotated[List[Dict[str, Any]], add] = Field(default_factory=list)\n",
        "    intent: Optional[Literal[\"rag\",\"order_status\",\"return_create\",\"ticket\",\"human_escalation\",\"unknown\"]] = None\n",
        "    answer: Optional[str] = None\n",
        "\n",
        "# ---- Routing ----\n",
        "def route_intent(state: AgentState) -> Dict[str, Any]:\n",
        "    q = state.user_input\n",
        "    low = (q or \"\").lower()\n",
        "    sentiment = detect_sentiment(q)\n",
        "    dept = classify_department(q)\n",
        "\n",
        "    if sentiment == \"negative\":\n",
        "        intent = \"human_escalation\"\n",
        "    elif any(kw in low for kw in [\"order status\", \"track order\", \"where is my order\", \"order tracking\"]):\n",
        "        intent = \"order_status\"\n",
        "    elif any(kw in low for kw in [\"return\", \"refund\", \"replace\", \"exchange\"]):\n",
        "        intent = \"return_create\"\n",
        "    elif any(kw in low for kw in [\"ticket\", \"helpdesk\", \"support issue\", \"complaint\", \"problem\"]):\n",
        "        intent = \"ticket\"\n",
        "    else:\n",
        "        intent = \"rag\"\n",
        "\n",
        "    print(f\"[route_intent] input={q!r} -> intent={intent}, dept={dept}, sentiment={sentiment}\")\n",
        "    return {\"intent\": intent, \"department\": dept, \"sentiment\": sentiment}\n",
        "\n",
        "# ---- Tool Node ----\n",
        "def _postfilter_by_dept(docs: List[Any], dept: Optional[str]) -> List[Any]:\n",
        "    if not docs:\n",
        "        return []\n",
        "    if dept is None:\n",
        "        return docs\n",
        "    filtered = [d for d in docs if (d.metadata or {}).get(\"department\") == dept]\n",
        "    return filtered or docs\n",
        "\n",
        "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
        "    intent = state.intent\n",
        "    q = state.user_input or \"\"\n",
        "    dept = state.department\n",
        "    print(f\"[tool_node] intent={intent}, dept={dept}, input={q!r}\")\n",
        "\n",
        "    if intent == \"order_status\":\n",
        "        return {\"answer\": \"Your order is being processed and will be shipped soon.\", \"tools_used\": [\"order_status_tool\"]}\n",
        "\n",
        "    if intent == \"return_create\":\n",
        "        return {\"answer\": \"Return initiated. You will receive pickup and label details via email.\", \"tools_used\": [\"return_create_tool\"]}\n",
        "\n",
        "    if intent == \"ticket\":\n",
        "        return {\"answer\": \"A support ticket has been created. Someone will get back to you shortly.\", \"tools_used\": [\"ticket_tool\"]}\n",
        "\n",
        "    if intent == \"human_escalation\":\n",
        "        return {\"answer\": \"I’m sorry for the inconvenience. Escalating to human support — someone will reach out to you soon.\", \"tools_used\": [\"escalation\"]}\n",
        "\n",
        "    if intent == \"rag\":\n",
        "        results = vector_store.similarity_search(q, k=5)\n",
        "        results = _postfilter_by_dept(results, dept)\n",
        "        if results:\n",
        "            top = results[0]\n",
        "            dept_meta = (top.metadata or {}).get(\"department\", \"unknown\")\n",
        "            return {\n",
        "                \"answer\": f\"{top.page_content} (Dept: {dept_meta})\",\n",
        "                \"tools_used\": [\"rag_retrieval\"],\n",
        "                \"retrieved\": [{\"content\": top.page_content, \"source\": dept_meta}]\n",
        "            }\n",
        "        else:\n",
        "            return {\"answer\": \"Sorry, no relevant info found in our knowledge base.\", \"tools_used\": [\"rag_retrieval\"]}\n",
        "\n",
        "    return {\"answer\": \"Could you please rephrase your request?\", \"tools_used\": [\"fallback\"]}\n",
        "\n",
        "# ---- Synthesis Node ----\n",
        "def synthesis_node(state: AgentState) -> Dict[str, Any]:\n",
        "    return {}\n",
        "\n",
        "# ---- Build Graph ----\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"route\", route_intent)\n",
        "graph.add_node(\"tool\", tool_node)\n",
        "graph.add_node(\"synth\", synthesis_node)\n",
        "\n",
        "graph.add_edge(START, \"route\")\n",
        "graph.add_edge(\"route\", \"tool\")\n",
        "graph.add_edge(\"tool\", \"synth\")\n",
        "graph.add_edge(\"synth\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)\n",
        "\n",
        "# ---- Ask Function ----\n",
        "def ask(q: str, thread_id: Optional[str] = None) -> str:\n",
        "    if thread_id is None:\n",
        "        import uuid\n",
        "        thread_id = f\"thread_{uuid.uuid4().hex}\"\n",
        "    out = app.invoke({\"user_input\": q},\n",
        "                     config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "    return out.get(\"answer\", \"No answer generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18bhUnZECRuH",
        "outputId": "e62e8772-6037-46b3-efc2-a7fbc5726934"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 75 FAQs from /content/shopunow_faqs.jsonl\n",
            "✅ Vector store ready with 75 documents across 5 departments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell B.1 - Testing\n",
        "# =========================\n",
        "\n",
        "import uuid\n",
        "\n",
        "for q in [\n",
        "    \"What are your support hours?\",\n",
        "    \"Tell me order status for order id ORD-1234\",\n",
        "    \"I want a return because the product is wrong\",\n",
        "    \"My password reset isn't working, this is frustrating\",\n",
        "    \"I submitted a complaint about a support issue\",\n",
        "    \"How do I pay with UPI?\",\n",
        "    \"Please replace my shirt size\",\n",
        "    \"I need help,\"  # ambiguous but should go ticket/support\n",
        "]:\n",
        "    # Create fresh thread_id each iteration\n",
        "    thread_id = f\"thread_{uuid.uuid4().hex}\"\n",
        "    state = AgentState(user_input=q)\n",
        "    info = route_intent(state)\n",
        "    print(f\"{q} → intent: {info['intent']} | dept: {info['department']} | sentiment: {info['sentiment']}\")\n",
        "    print(\"Answer:\", ask(q, thread_id=thread_id))\n",
        "    print(\"----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL6WcVgTCuGf",
        "outputId": "1af61d5f-ba04-42e3-fcb5-6d0a28de7751"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[route_intent] input='What are your support hours?' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "What are your support hours? → intent: rag | dept: Customer Support | sentiment: positive\n",
            "[route_intent] input='What are your support hours?' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "[tool_node] intent=rag, dept=Customer Support, input='What are your support hours?'\n",
            "Answer: Currently, support is available from 9 AM to 9 PM IST, Monday through Saturday. Emergency queries can be raised via the support portal. (Dept: Customer Support)\n",
            "----\n",
            "[route_intent] input='Tell me order status for order id ORD-1234' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "Tell me order status for order id ORD-1234 → intent: order_status | dept: Orders & Returns | sentiment: neutral\n",
            "[route_intent] input='Tell me order status for order id ORD-1234' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, input='Tell me order status for order id ORD-1234'\n",
            "Answer: Your order is being processed and will be shipped soon.\n",
            "----\n",
            "[route_intent] input='I want a return because the product is wrong' -> intent=human_escalation, dept=Orders & Returns, sentiment=negative\n",
            "I want a return because the product is wrong → intent: human_escalation | dept: Orders & Returns | sentiment: negative\n",
            "[route_intent] input='I want a return because the product is wrong' -> intent=human_escalation, dept=Orders & Returns, sentiment=negative\n",
            "[tool_node] intent=human_escalation, dept=Orders & Returns, input='I want a return because the product is wrong'\n",
            "Answer: I’m sorry for the inconvenience. Escalating to human support — someone will reach out to you soon.\n",
            "----\n",
            "[route_intent] input=\"My password reset isn't working, this is frustrating\" -> intent=human_escalation, dept=HR & IT Helpdesk, sentiment=negative\n",
            "My password reset isn't working, this is frustrating → intent: human_escalation | dept: HR & IT Helpdesk | sentiment: negative\n",
            "[route_intent] input=\"My password reset isn't working, this is frustrating\" -> intent=human_escalation, dept=HR & IT Helpdesk, sentiment=negative\n",
            "[tool_node] intent=human_escalation, dept=HR & IT Helpdesk, input=\"My password reset isn't working, this is frustrating\"\n",
            "Answer: I’m sorry for the inconvenience. Escalating to human support — someone will reach out to you soon.\n",
            "----\n",
            "[route_intent] input='I submitted a complaint about a support issue' -> intent=ticket, dept=Customer Support, sentiment=neutral\n",
            "I submitted a complaint about a support issue → intent: ticket | dept: Customer Support | sentiment: neutral\n",
            "[route_intent] input='I submitted a complaint about a support issue' -> intent=ticket, dept=Customer Support, sentiment=neutral\n",
            "[tool_node] intent=ticket, dept=Customer Support, input='I submitted a complaint about a support issue'\n",
            "Answer: A support ticket has been created. Someone will get back to you shortly.\n",
            "----\n",
            "[route_intent] input='How do I pay with UPI?' -> intent=rag, dept=Payments & Billing, sentiment=neutral\n",
            "How do I pay with UPI? → intent: rag | dept: Payments & Billing | sentiment: neutral\n",
            "[route_intent] input='How do I pay with UPI?' -> intent=rag, dept=Payments & Billing, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=Payments & Billing, input='How do I pay with UPI?'\n",
            "Answer: We accept UPI, debit/credit cards, wallets, and cash on delivery. (Dept: Payments & Billing)\n",
            "----\n",
            "[route_intent] input='Please replace my shirt size' -> intent=return_create, dept=Orders & Returns, sentiment=positive\n",
            "Please replace my shirt size → intent: return_create | dept: Orders & Returns | sentiment: positive\n",
            "[route_intent] input='Please replace my shirt size' -> intent=return_create, dept=Orders & Returns, sentiment=positive\n",
            "[tool_node] intent=return_create, dept=Orders & Returns, input='Please replace my shirt size'\n",
            "Answer: Return initiated. You will receive pickup and label details via email.\n",
            "----\n",
            "[route_intent] input='I need help,' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "I need help, → intent: rag | dept: Customer Support | sentiment: positive\n",
            "[route_intent] input='I need help,' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "[tool_node] intent=rag, dept=Customer Support, input='I need help,'\n",
            "Answer: Support can guide you to product manuals or connect you to brand-specific helplines. (Dept: Customer Support)\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell C - Flask API with ngrok (collision-safe, deep debug, auto-reuse)\n",
        "# =========================\n",
        "import os, sys, traceback, threading, uuid\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "# --------- Flask Setup (separate var from LangGraph 'app') ---------\n",
        "flask_app = Flask(__name__)\n",
        "CORS(flask_app)\n",
        "\n",
        "def _debug(msg):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# --------- Agent Caller ---------\n",
        "def call_agent(query: str) -> str:\n",
        "    if \"ask\" in globals() and callable(globals()[\"ask\"]):\n",
        "        _debug(f\"[AGENT] Using ask()\")\n",
        "        return globals()[\"ask\"](query)\n",
        "\n",
        "    for name in [\"agent_app\", \"graph_app\", \"app\"]:\n",
        "        obj = globals().get(name)\n",
        "        if hasattr(obj, \"invoke\"):\n",
        "            _debug(f\"[AGENT] Using graph '{name}'.invoke()\")\n",
        "            cfg = {\"configurable\": {\"thread_id\": f\"api-{uuid.uuid4().hex}\"}}\n",
        "            out = obj.invoke({\"user_input\": query}, config=cfg)\n",
        "            return out.get(\"answer\", \"No answer generated.\")\n",
        "    raise RuntimeError(\"No agent available. Run Cell B first.\")\n",
        "\n",
        "# --------- Routes ---------\n",
        "@flask_app.route(\"/ask\", methods=[\"POST\", \"GET\"])\n",
        "def ask_api():\n",
        "    try:\n",
        "        _debug(\"\\n[API] ▶️ Received /ask\")\n",
        "        if request.method == \"POST\":\n",
        "            if not request.is_json:\n",
        "                return jsonify({\"error\": \"Content-Type must be application/json\"}), 400\n",
        "            data = request.get_json(force=True, silent=True) or {}\n",
        "            query = (data.get(\"query\") or \"\").strip()\n",
        "        else:\n",
        "            query = (request.args.get(\"query\") or \"\").strip()\n",
        "\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Empty query\"}), 400\n",
        "\n",
        "        answer = call_agent(query)\n",
        "        return jsonify({\"query\": query, \"answer\": answer})\n",
        "    except Exception as e:\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "        return jsonify({\"error\": \"Internal server error\", \"details\": str(e)}), 500\n",
        "\n",
        "@flask_app.route(\"/\", methods=[\"GET\"])\n",
        "def home():\n",
        "    return jsonify({\"status\": \"ok\", \"message\": \"ShopUNow Agent API is running!\"})\n",
        "\n",
        "# --------- Run Flask (dynamic port, avoids collisions) ---------\n",
        "from werkzeug.serving import make_server\n",
        "import socket\n",
        "\n",
        "def find_free_port(default=5000):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind((\"\", 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "PORT = find_free_port(5000)\n",
        "_debug(f\"▶️ Starting Flask server on port {PORT}...\")\n",
        "\n",
        "def run_flask():\n",
        "    try:\n",
        "        flask_app.run(host=\"0.0.0.0\", port=PORT, debug=False, use_reloader=False)\n",
        "    except Exception as e:\n",
        "        _debug(f\"❌ Flask crashed: {e}\")\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "threading.Thread(target=run_flask, daemon=True).start()\n",
        "\n",
        "# --------- ngrok setup ---------\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "except ImportError:\n",
        "    _debug(\"[NGROK] Installing pyngrok...\")\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyngrok\"], check=True)\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "def _get_secret(name):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(name)\n",
        "    except Exception:\n",
        "        return os.getenv(name)\n",
        "\n",
        "NGROK_AUTH_TOKEN = _get_secret(\"NGROK_AUTH_TOKEN\")\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    _debug(\"🔑 Setting ngrok auth token\")\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Kill old tunnels before opening a new one\n",
        "ngrok.kill()\n",
        "\n",
        "try:\n",
        "    _debug(f\"🌐 Starting ngrok tunnel on :{PORT} ...\")\n",
        "    tunnel = ngrok.connect(PORT)\n",
        "    public_url = getattr(tunnel, \"public_url\", str(tunnel))\n",
        "    _debug(f\"🚀 Public API URL: {public_url}\")\n",
        "    print(\"\\nTest with:\")\n",
        "    print(f'curl -X POST \"{public_url}/ask\" -H \"Content-Type: application/json\" -d \"{{\\\\\"query\\\\\": \\\\\"What are your support hours?\\\\\"}}\"')\n",
        "except Exception as e:\n",
        "    _debug(f\"❌ ngrok connection failed: {e}\")\n",
        "    traceback.print_exc(file=sys.stdout)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgBZx70SRRCQ",
        "outputId": "73b9ccc6-ad90-48c2-c0ac-e4636ca08392"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▶️ Starting Flask server on port 60179...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:60179\n",
            " * Running on http://172.28.0.12:60179\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Setting ngrok auth token\n",
            "🌐 Starting ngrok tunnel on :60179 ...\n",
            "🚀 Public API URL: https://31a863ccbd3a.ngrok-free.app\n",
            "\n",
            "Test with:\n",
            "curl -X POST \"https://31a863ccbd3a.ngrok-free.app/ask\" -H \"Content-Type: application/json\" -d \"{\\\"query\\\": \\\"What are your support hours?\\\"}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell D - Streamlit Frontend (Mandatory Keys + LangSmith optional)\n",
        "# =========================\n",
        "import subprocess, threading\n",
        "!pip install -q streamlit requests pyngrok\n",
        "\n",
        "with open(\"app_frontend.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "st.set_page_config(page_title=\"ShopUNow Agent\", page_icon=\"🛍️\", layout=\"centered\")\n",
        "st.title(\"🛍️ ShopUNow AI Assistant\")\n",
        "\n",
        "# --- Sidebar: Configuration ---\n",
        "st.sidebar.header(\"🔑 Configuration (Required)\")\n",
        "\n",
        "api_url = st.sidebar.text_input(\"Flask API URL (ngrok/public)\", value=\"http://127.0.0.1:5000/ask\")\n",
        "openai_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "gemini_key = st.sidebar.text_input(\"Gemini API Key\", type=\"password\")\n",
        "ngrok_token = st.sidebar.text_input(\"ngrok Auth Token\", type=\"password\")\n",
        "\n",
        "st.sidebar.header(\"Optional\")\n",
        "langsmith_key = st.sidebar.text_input(\"LangSmith Key (optional)\", type=\"password\")\n",
        "\n",
        "# ---- Validation ----\n",
        "errors = []\n",
        "if not api_url.strip():\n",
        "    errors.append(\"❌ Flask API URL is required.\")\n",
        "if not (openai_key.strip() or gemini_key.strip()):\n",
        "    errors.append(\"❌ At least one model key (OpenAI or Gemini) is required.\")\n",
        "if not ngrok_token.strip():\n",
        "    errors.append(\"❌ ngrok Auth Token is required.\")\n",
        "\n",
        "if errors:\n",
        "    for e in errors:\n",
        "        st.sidebar.error(e)\n",
        "    st.stop()\n",
        "else:\n",
        "    st.sidebar.success(\"✅ All required configuration set\")\n",
        "\n",
        "# Store secrets\n",
        "st.session_state.secrets = {\n",
        "    \"API_URL\": api_url.strip(),\n",
        "    \"OPENAI_API_KEY\": openai_key.strip(),\n",
        "    \"GEMINI_API_KEY\": gemini_key.strip(),\n",
        "    \"NGROK_AUTH_TOKEN\": ngrok_token.strip(),\n",
        "    \"LANGSMITH_KEY\": langsmith_key.strip(),\n",
        "}\n",
        "\n",
        "# --- Chat Section ---\n",
        "st.subheader(\"💬 Chat with ShopUNow Agent\")\n",
        "\n",
        "if \"chat\" not in st.session_state:\n",
        "    st.session_state.chat = []\n",
        "\n",
        "query = st.text_input(\"Type your question here:\")\n",
        "\n",
        "if st.button(\"Ask\"):\n",
        "    if query.strip():\n",
        "        st.session_state.chat.append((\"🧑 You\", query))\n",
        "        try:\n",
        "            resp = requests.post(st.session_state.secrets[\"API_URL\"], json={\"query\": query}, timeout=20)\n",
        "            if resp.status_code == 200:\n",
        "                data = resp.json()\n",
        "                answer = data.get(\"answer\", \"⚠️ No answer returned\")\n",
        "            else:\n",
        "                answer = f\"⚠️ Error {resp.status_code}: {resp.text}\"\n",
        "        except Exception as e:\n",
        "            answer = f\"⚠️ Request failed: {e}\"\n",
        "\n",
        "        st.session_state.chat.append((\"🤖 Agent\", answer))\n",
        "\n",
        "# --- Display chat history ---\n",
        "for sender, msg in st.session_state.chat:\n",
        "    st.markdown(f\"**{sender}:** {msg}\")\n",
        "\"\"\")\n",
        "\n",
        "# ---- Run Streamlit in background ----\n",
        "def run_streamlit():\n",
        "    subprocess.run(\n",
        "        [\"streamlit\", \"run\", \"app_frontend.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"]\n",
        "    )\n",
        "\n",
        "threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "\n",
        "# ---- ngrok tunnel for frontend ----\n",
        "from pyngrok import ngrok\n",
        "print(\"🌐 Starting ngrok tunnel for Streamlit...\")\n",
        "frontend_url = ngrok.connect(8501)\n",
        "print(\"🚀 Streamlit Frontend URL:\", frontend_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgocOgNHW72d",
        "outputId": "086007b4-a2d8-4f8c-ee00-80d3fd35e1a4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Starting ngrok tunnel for Streamlit...\n",
            "🚀 Streamlit Frontend URL: NgrokTunnel: \"https://df18f55c7b61.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}