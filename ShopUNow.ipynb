{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkINmjKJDIyvye6DFpqJzU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-disha/ShopUNow-Agent/blob/main/ShopUNow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####################################\n"
      ],
      "metadata": {
        "id": "iEqj7kzsa8Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tech Stack\n",
        "\n",
        "LangChain / langchain_community-\tProvides VectorStores (FAISS), Document abstraction, Embeddings, and Retrieval.\n",
        "FAISS (vectorstore)-\tFor embedding storage & similarity search (RAG).\n",
        "Sentence-Transformers embeddings-\tTo convert document chunks into embedding vectors.\n",
        "**pdfminer.six + pytesseract + PIL**-\tExtract text from PDFs, images (OCR) and markdown/text files — for building corpus.\n",
        "Markdownify\tConvert markdown files to plain text.\n",
        "LangGraph (StateGraph etc.)-\tThe agent orchestration framework: state + nodes + transitions.\n",
        "Pydantic-\tFor structured schemas of state and tool inputs (validation, typing).\n",
        "LLM backends- OpenAI, Gemini (if available)\tFor synthesis / general LLM responses."
      ],
      "metadata": {
        "id": "lcNx5EjDfTQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parses different document types (text, csv, pdf, image) into a corpus.**\n",
        "\n",
        "**Chunks documents into manageable pieces using RecursiveCharacterTextSplitter.**\n",
        "\n",
        "**Builds a FAISS index, persists it.**\n",
        "\n",
        "**Sets up intent routing + tools for order status, returns, tickets.**\n",
        "\n",
        "**Handles RAG retrieval + LLM synthesis with system prompt.**\n",
        "\n",
        "**Passes retriever via RunnableConfig/configurable, avoiding earlier bug.**\n",
        "\n",
        "**Good structure using StateGraph, Pydantic state schemas.**"
      ],
      "metadata": {
        "id": "e27iQa2Rezzd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d82ad50"
      },
      "source": [
        "# Task\n",
        "Modify the code to use Gemini as the primary LLM and fallback to OpenAI, and add tools for order status, returns, and tickets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f6e900c"
      },
      "source": [
        "## Integrate llms (gemini and openai)\n",
        "\n",
        "### Subtask:\n",
        "Modify the agent to use a language model for answer synthesis, with Gemini as the primary and OpenAI as a fallback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091b1bad"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `get_chat_model` function to handle primary (Gemini) and fallback (OpenAI) LLM initialization and then update the `synthesis_node` to use this LLM for answer generation based on context and user input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7121c35"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `langchain_google_genai` was not installed. Install the missing package and try the imports and function definition again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell A: Setup and Vector Store + LLM\n",
        "# =========================\n",
        "\n",
        "# Install required packages (include Google-GenAI integration if using Gemini)\n",
        "!pip install -qU langchain_community faiss-cpu langchain_openai langchain-google-genai pydantic typing_extensions vaderSentiment langgraph\n",
        "\n",
        "!pip install -qU flask flask-cors pyngrok\n",
        "\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional, Literal\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Make sure API keys are set\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    if key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    gem_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "    if gem_key:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = gem_key\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "GEMINI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")  # for Gemini\n",
        "\n",
        "assert OPENAI_API_KEY or GEMINI_API_KEY, \"Please set OPENAI_API_KEY or GEMINI_API_KEY in environment variables.\"\n",
        "\n",
        "# Initialize embeddings (use OpenAI embeddings; you can use Gemini embeddings if you want and have the key)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Sample documents / FAQ data with department metadata\n",
        "# You should expand these to 10-15 QA per department later\n",
        "faq_docs = [\n",
        "    Document(page_content=\"Support hours are 9 AM–9 PM IST, Monday to Saturday\", metadata={\"department\": \"Customer Support\"}),\n",
        "    Document(page_content=\"How to contact support email or phone\", metadata={\"department\": \"Customer Support\"}),\n",
        "    Document(page_content=\"Return window is 10 days from delivery\", metadata={\"department\": \"Orders & Returns\"}),\n",
        "    Document(page_content=\"How can I initiate a return process\", metadata={\"department\": \"Orders & Returns\"}),\n",
        "    Document(page_content=\"We accept UPI, credit cards, wallets, and COD\", metadata={\"department\": \"Payments & Billing\"}),\n",
        "    Document(page_content=\"How to apply coupon at checkout\", metadata={\"department\": \"Payments & Billing\"})\n",
        "]\n",
        "\n",
        "# Build the FAISS vector store\n",
        "import faiss\n",
        "\n",
        "# Compute embedding dimension\n",
        "dim = len(embeddings.embed_query(\"hello world\"))  # length of vector\n",
        "\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore({}),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "ids = [f\"doc{i+1}\" for i in range(len(faq_docs))]\n",
        "vector_store.add_documents(documents=faq_docs, ids=ids)\n",
        "\n",
        "# Initialize LLM (Chat model)\n",
        "def get_chat_model():\n",
        "    if GEMINI_API_KEY:\n",
        "        try:\n",
        "            print(\"Using Gemini LLM\")\n",
        "            return ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
        "        except Exception as e:\n",
        "            print(\"Gemini init failed:\", e)\n",
        "    if OPENAI_API_KEY:\n",
        "        try:\n",
        "            print(\"Using OpenAI model\")\n",
        "            return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "        except Exception as e:\n",
        "            print(\"OpenAI init failed:\", e)\n",
        "    # fallback mock model\n",
        "    class _Mock:\n",
        "        def invoke(self, messages: List[Any]):\n",
        "            last_user = None\n",
        "            for m in reversed(messages):\n",
        "                if isinstance(m, HumanMessage):\n",
        "                    last_user = m\n",
        "                    break\n",
        "            return type(\"Obj\", (), {\"content\": \"[MOCK] \" + (last_user.content if last_user else \"\")})\n",
        "    print(\"Using mock LLM fallback\")\n",
        "    return _Mock()\n",
        "\n",
        "LLM = get_chat_model()\n",
        "\n",
        "# Optionally define system policy / prompt template\n",
        "SYSTEM_POLICY = (\n",
        "    \"You are ShopUNow Assistant. Be concise and accurate. Use the internal knowledge base when possible. \"\n",
        "    \"Cite sources. If you cannot answer, ask a clarifying question.\"\n",
        ")\n",
        "\n",
        "print(\"Cell A setup complete: vector_store and LLM are initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIkC2RVqCKeo",
        "outputId": "2e5b469e-9057-4a94-fd0b-cdff9bb37e42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Gemini LLM\n",
            "Cell A setup complete: vector_store and LLM are initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell A.1 - Load FAQ Dataset & Build Vector Store\n",
        "# =========================\n",
        "import json\n",
        "import faiss\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# ---- Step 1: Load JSONL file ----\n",
        "faq_path = \"/content/shopunow_faqs.jsonl\"\n",
        "faq_docs = []\n",
        "with open(faq_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:   # skip empty lines\n",
        "            continue\n",
        "        try:\n",
        "            record = json.loads(line)\n",
        "            question = record.get(\"question\", \"\").strip()\n",
        "            answer = record.get(\"answer\", \"\").strip()\n",
        "            dept = record.get(\"department\", \"unknown\").strip()\n",
        "\n",
        "            # Store Q + A in embeddings for better recall\n",
        "            combined_text = f\"Q: {question}\\nA: {answer}\"\n",
        "\n",
        "            faq_docs.append(\n",
        "                Document(\n",
        "                    page_content=combined_text,\n",
        "                    metadata={\n",
        "                        \"department\": dept,\n",
        "                        \"question\": question,\n",
        "                        \"answer\": answer\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"⚠️ Skipping bad line: {line[:80]}... | Error: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(faq_docs)} FAQs from {faq_path}\")\n",
        "\n",
        "# ---- Step 2: Build FAISS Vector Store ----\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "embedding_dim = len(embedding_model.embed_query(\"hello world\"))\n",
        "\n",
        "# Use cosine similarity (inner product) instead of L2\n",
        "faiss_index = faiss.IndexFlatIP(embedding_dim)\n",
        "\n",
        "faq_vector_store = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=faiss_index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "faq_ids = [f\"faq_{i+1}\" for i in range(len(faq_docs))]\n",
        "faq_vector_store.add_documents(documents=faq_docs, ids=faq_ids)\n",
        "\n",
        "print(f\"✅ Vector store built with {len(faq_docs)} FAQs \"\n",
        "      f\"across {len(set(d.metadata['department'] for d in faq_docs))} departments\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n48dlld5MHiC",
        "outputId": "82d33577-80cb-4256-e99c-2d1572f38103"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 119 FAQs from /content/shopunow_faqs.jsonl\n",
            "✅ Vector store built with 119 FAQs across 5 departments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell B - Agent Definition with JSONL Vector Store\n",
        "# (cosine similarity + dept thresholds + fuzzy fallback + deterministic seeds)\n",
        "# =========================\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import Optional, List, Dict, Any, Literal\n",
        "from typing_extensions import Annotated\n",
        "from operator import add\n",
        "from pydantic import BaseModel, Field\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# ---- Deterministic seeds ----\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---- Load FAQ dataset from JSONL ----\n",
        "jsonl_path = \"/content/shopunow_faqs.jsonl\"\n",
        "faq_documents = []\n",
        "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        record = json.loads(line)\n",
        "        faq_documents.append(\n",
        "            Document(\n",
        "                page_content=f\"Q: {record['question']}\\nA: {record['answer']}\",\n",
        "                metadata={\n",
        "                    \"department\": record[\"department\"],\n",
        "                    \"question\": record[\"question\"],\n",
        "                    \"answer\": record[\"answer\"]\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "print(f\"Loaded {len(faq_documents)} FAQs from {jsonl_path}\")\n",
        "\n",
        "# ---- Normalized Embeddings (cosine similarity) ----\n",
        "class NormalizedOpenAIEmbeddings(OpenAIEmbeddings):\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        v = np.array(super().embed_query(text))\n",
        "        return (v / (np.linalg.norm(v) + 1e-10)).tolist()\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        vs = np.array(super().embed_documents(texts))\n",
        "        return (vs / (np.linalg.norm(vs, axis=1, keepdims=True) + 1e-10)).tolist()\n",
        "\n",
        "embedding_model = NormalizedOpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "dim = len(embedding_model.embed_query(\"hello world\"))\n",
        "faiss_index = faiss.IndexFlatIP(dim)  # cosine similarity via inner product\n",
        "\n",
        "faq_vector_store = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=faiss_index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "faq_ids = [f\"faq_{i+1}\" for i in range(len(faq_documents))]\n",
        "faq_vector_store.add_documents(documents=faq_documents, ids=faq_ids)\n",
        "\n",
        "print(f\"✅ Vector store ready with {len(faq_documents)} documents \"\n",
        "      f\"across {len(set(d.metadata['department'] for d in faq_documents))} departments\")\n",
        "\n",
        "# ---- Sentiment + Dept Classifier ----\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def detect_sentiment(text: str) -> Literal[\"negative\",\"neutral\",\"positive\"]:\n",
        "    if not text:\n",
        "        return \"neutral\"\n",
        "    scores = sentiment_analyzer.polarity_scores(text)\n",
        "    compound = scores.get(\"compound\", 0.0)\n",
        "    if compound <= -0.3: return \"negative\"\n",
        "    if compound >= 0.3: return \"positive\"\n",
        "    return \"neutral\"\n",
        "\n",
        "def classify_department(user_query: str) -> Optional[str]:\n",
        "    query = (user_query or \"\").lower()\n",
        "    if any(kw in query for kw in [\"order status\", \"track order\", \"where is my order\", \"order tracking\", \"delayed\"]):\n",
        "        return \"Orders & Returns\"\n",
        "    if any(kw in query for kw in [\"return\", \"refund\", \"replace\", \"exchange\"]):\n",
        "        return \"Orders & Returns\"\n",
        "    if any(kw in query for kw in [\"payment\", \"upi\", \"card\", \"wallet\", \"cod\", \"invoice\", \"coupon\"]):\n",
        "        return \"Payments & Billing\"\n",
        "    if any(kw in query for kw in [\"support\", \"contact\", \"help\", \"issue\", \"complaint\", \"hours\"]):\n",
        "        return \"Customer Support\"\n",
        "    if any(kw in query for kw in [\"password\", \"vpn\", \"access\", \"onboarding\", \"hardware\",\n",
        "                                  \"software\", \"leave\", \"policy\"]):\n",
        "        return \"HR & IT Helpdesk\"\n",
        "    return None\n",
        "\n",
        "# ---- Dept-specific similarity thresholds ----\n",
        "DEPT_THRESHOLDS = {\n",
        "    \"Orders & Returns\": 0.80,\n",
        "    \"Payments & Billing\": 0.78,\n",
        "    \"Customer Support\": 0.75,\n",
        "    \"HR & IT Helpdesk\": 0.70,\n",
        "    None: 0.76  # default\n",
        "}\n",
        "\n",
        "# ---- Agent State ----\n",
        "class AgentState(BaseModel):\n",
        "    user_input: str\n",
        "    department: Optional[str] = None\n",
        "    sentiment: Optional[Literal[\"negative\",\"neutral\",\"positive\"]] = None\n",
        "    tools_used: Annotated[List[str], add] = Field(default_factory=list)\n",
        "    retrieved: Annotated[List[Dict[str, Any]], add] = Field(default_factory=list)\n",
        "    intent: Optional[Literal[\"rag\",\"order_status\",\"return_create\",\n",
        "                             \"ticket\",\"human_escalation\",\"unknown\"]] = None\n",
        "    answer: Optional[str] = None\n",
        "\n",
        "# ---- Routing ----\n",
        "def route_intent(state: AgentState) -> Dict[str, Any]:\n",
        "    user_query = state.user_input\n",
        "    query_lower = (user_query or \"\").lower()\n",
        "    sentiment = detect_sentiment(user_query)\n",
        "    predicted_department = classify_department(user_query)\n",
        "\n",
        "    # Negative sentiment → escalate\n",
        "    if sentiment == \"negative\":\n",
        "        intent = \"human_escalation\"\n",
        "    # Explicit rules\n",
        "    elif any(kw in query_lower for kw in [\"order status\", \"track order\", \"where is my order\", \"order tracking\"]):\n",
        "        intent = \"order_status\"\n",
        "    elif \"delayed\" in query_lower and \"return\" not in query_lower:\n",
        "        intent = \"order_status\"\n",
        "    elif any(kw in query_lower for kw in [\"return\", \"refund\", \"replace\", \"exchange\"]):\n",
        "        # If asking about return *policy* or days → informational\n",
        "        if \"policy\" in query_lower or \"how many\" in query_lower or \"days\" in query_lower:\n",
        "            intent = \"rag\"\n",
        "        else:\n",
        "            intent = \"return_create\"\n",
        "    elif any(kw in query_lower for kw in [\"ticket\", \"helpdesk\", \"support issue\", \"complaint\", \"problem\"]):\n",
        "        intent = \"ticket\"\n",
        "    else:\n",
        "        intent = \"rag\"\n",
        "\n",
        "    print(f\"[route_intent] input={user_query!r} -> intent={intent}, dept={predicted_department}, sentiment={sentiment}\")\n",
        "    return {\"intent\": intent, \"department\": predicted_department, \"sentiment\": sentiment}\n",
        "\n",
        "# ---- Tool Node ----\n",
        "def filter_by_department(results: List[Any], predicted_department: Optional[str]) -> List[Any]:\n",
        "    if not results:\n",
        "        return []\n",
        "    if predicted_department is None:\n",
        "        return results\n",
        "    filtered = [(doc, score) for doc, score in results\n",
        "                if (doc.metadata or {}).get(\"department\") == predicted_department]\n",
        "    return filtered or results\n",
        "\n",
        "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
        "    intent, user_query, predicted_department = state.intent, state.user_input or \"\", state.department\n",
        "    print(f\"[tool_node] intent={intent}, dept={predicted_department}, input={user_query!r}\")\n",
        "\n",
        "    # --- Direct tool responses ---\n",
        "    if intent == \"order_status\":\n",
        "        return {\"answer\": \"Your order is being processed and will be shipped soon.\",\n",
        "                \"tools_used\": [\"order_status_tool\"]}\n",
        "    if intent == \"return_create\":\n",
        "        return {\"answer\": \"Return initiated. You will receive pickup and label details via email.\",\n",
        "                \"tools_used\": [\"return_create_tool\"]}\n",
        "    if intent == \"ticket\":\n",
        "        return {\"answer\": \"A support ticket has been created. Someone will get back to you shortly.\",\n",
        "                \"tools_used\": [\"ticket_tool\"]}\n",
        "    if intent == \"human_escalation\":\n",
        "        return {\"answer\": \"I’m sorry for the inconvenience. Escalating to human support — \"\n",
        "                          \"someone will reach out to you soon.\",\n",
        "                \"tools_used\": [\"escalation\"]}\n",
        "\n",
        "    # --- Retrieval (RAG) ---\n",
        "    if intent == \"rag\":\n",
        "        try:\n",
        "            results = faq_vector_store.similarity_search_with_score(user_query, k=5)\n",
        "            results = [(doc, score) for doc, score in results if doc is not None]\n",
        "            results = filter_by_department(results, predicted_department)\n",
        "\n",
        "            if results:\n",
        "                top_doc, cosine_similarity = results[0]\n",
        "                print(f\"[tool_node] Top cosine similarity={cosine_similarity:.4f}\")\n",
        "                threshold = DEPT_THRESHOLDS.get(predicted_department, DEPT_THRESHOLDS[None])\n",
        "\n",
        "                # Below threshold → try fuzzy match\n",
        "                if cosine_similarity < threshold:\n",
        "                    best_doc, best_fuzzy_score = None, 0\n",
        "                    for doc in faq_documents:\n",
        "                        fuzzy_score = fuzz.partial_ratio(user_query.lower(), doc.metadata[\"question\"].lower()) / 100.0\n",
        "                        if fuzzy_score > best_fuzzy_score:\n",
        "                            best_doc, best_fuzzy_score = doc, fuzzy_score\n",
        "                    if best_doc and best_fuzzy_score >= 0.90:\n",
        "                        dept_meta = best_doc.metadata.get(\"department\", \"unknown\")\n",
        "                        answer_text = best_doc.metadata.get(\"answer\", best_doc.page_content)\n",
        "                        return {\"answer\": f\"{answer_text} (Dept: {dept_meta})\",\n",
        "                                \"tools_used\": [\"rag_fuzzy_fallback\"],\n",
        "                                \"retrieved\": [{\"question\": best_doc.metadata.get(\"question\", \"\"),\n",
        "                                               \"answer\": answer_text,\n",
        "                                               \"fuzzy_score\": float(best_fuzzy_score),\n",
        "                                               \"source\": dept_meta}]}\n",
        "                    return {\"answer\": \"🤖 Sorry, I don’t know the exact answer to that. Escalating to human support.\",\n",
        "                            \"tools_used\": [\"rag_retrieval\"], \"retrieved\": []}\n",
        "\n",
        "                # Passed threshold\n",
        "                dept_meta = (top_doc.metadata or {}).get(\"department\", \"unknown\")\n",
        "                answer_text = top_doc.metadata.get(\"answer\", top_doc.page_content)\n",
        "                return {\"answer\": f\"{answer_text} (Dept: {dept_meta})\",\n",
        "                        \"tools_used\": [\"rag_retrieval\"],\n",
        "                        \"retrieved\": [{\"question\": top_doc.metadata.get(\"question\", \"\"),\n",
        "                                       \"answer\": answer_text,\n",
        "                                       \"similarity\": float(cosine_similarity),\n",
        "                                       \"source\": dept_meta}]}\n",
        "            else:\n",
        "                return {\"answer\": \"Sorry, no relevant info found in our knowledge base.\",\n",
        "                        \"tools_used\": [\"rag_retrieval\"]}\n",
        "        except Exception as e:\n",
        "            print(f\"[tool_node] ❌ Retrieval error: {e}\")\n",
        "            return {\"answer\": \"Sorry, something went wrong while searching the knowledge base.\",\n",
        "                    \"tools_used\": [\"rag_retrieval\"]}\n",
        "\n",
        "    return {\"answer\": \"Could you please rephrase your request?\", \"tools_used\": [\"fallback\"]}\n",
        "\n",
        "# ---- Synthesis Node ----\n",
        "def synthesis_node(state: AgentState) -> Dict[str, Any]:\n",
        "    return {}\n",
        "\n",
        "# ---- Build Graph ----\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"route\", route_intent)\n",
        "graph.add_node(\"tool\", tool_node)\n",
        "graph.add_node(\"synth\", synthesis_node)\n",
        "\n",
        "graph.add_edge(START, \"route\")\n",
        "graph.add_edge(\"route\", \"tool\")\n",
        "graph.add_edge(\"tool\", \"synth\")\n",
        "graph.add_edge(\"synth\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)\n",
        "\n",
        "# ---- Ask Function ----\n",
        "def ask(user_query: str, thread_id: Optional[str] = None) -> str:\n",
        "    if thread_id is None:\n",
        "        import uuid\n",
        "        thread_id = f\"thread_{uuid.uuid4().hex}\"\n",
        "    output = app.invoke({\"user_input\": user_query},\n",
        "                        config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "    return output.get(\"answer\", \"No answer generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18bhUnZECRuH",
        "outputId": "8b4bb300-dfd1-4f4f-81ce-7cf9869c14d1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 119 FAQs from /content/shopunow_faqs.jsonl\n",
            "✅ Vector store ready with 119 documents across 5 departments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell B.1 - Testing (Improved Output & Meaningful Names)\n",
        "# =========================\n",
        "import uuid\n",
        "from pprint import pprint\n",
        "\n",
        "test_queries = [\n",
        "    \"What are your support hours?\",\n",
        "    \"Tell me order status for order id ORD-1234\",\n",
        "    \"I want a return because the product is wrong\",\n",
        "    \"My password reset isn't working, this is frustrating\",\n",
        "    \"I submitted a complaint about a support issue\",\n",
        "    \"How do I pay with UPI?\",\n",
        "    \"How to apply for leaves?\",\n",
        "    \"what is the leave policy?\",\n",
        "    \"where is my order?\",\n",
        "    \"my order is delayed\",\n",
        "    \"whats the return polcy? how many days can i return the product?\",\n",
        "    \"how many leaves can I take?\",\n",
        "    \" how many days for rturn?\",\n",
        "    \"my retturn is delayed\",\n",
        "    \"Please replace my shirt size\",\n",
        "    \"I need help,\",  # ambiguous\n",
        "]\n",
        "\n",
        "for user_query in test_queries:\n",
        "    # Create a fresh conversation ID for isolation\n",
        "    conversation_id = f\"conv_{uuid.uuid4().hex}\"\n",
        "    state = AgentState(user_input=user_query)\n",
        "\n",
        "    # Route intent & department\n",
        "    route_info = route_intent(state)\n",
        "\n",
        "    # Run through the graph (ask function)\n",
        "    response = app.invoke(\n",
        "        {\"user_input\": user_query},\n",
        "        config={\"configurable\": {\"thread_id\": conversation_id}}\n",
        "    )\n",
        "\n",
        "    # Print structured logs\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"🧑 User Query: {user_query}\")\n",
        "    print(f\"➡️ Intent: {route_info['intent']} | Dept: {route_info['department']} | Sentiment: {route_info['sentiment']}\")\n",
        "    print(f\"🤖 Agent Answer: {response.get('answer', '⚠️ No answer')}\")\n",
        "    print(f\"🛠️ Tools Used: {response.get('tools_used')}\")\n",
        "    if response.get(\"retrieved\"):\n",
        "        print(\"📄 Retrieved Context:\")\n",
        "        pprint(response[\"retrieved\"])\n",
        "    print(\"=\" * 80 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL6WcVgTCuGf",
        "outputId": "8b74213e-d263-44c5-a4d9-dea5b9d31035"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[route_intent] input='What are your support hours?' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "[route_intent] input='What are your support hours?' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "[tool_node] intent=rag, dept=Customer Support, input='What are your support hours?'\n",
            "[tool_node] Top cosine similarity=0.8337\n",
            "================================================================================\n",
            "🧑 User Query: What are your support hours?\n",
            "➡️ Intent: rag | Dept: Customer Support | Sentiment: positive\n",
            "🤖 Agent Answer: Yes, live chat is available from 9 AM to 9 PM IST on our website and mobile app. (Dept: Customer Support)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': 'Yes, live chat is available from 9 AM to 9 PM IST on our website '\n",
            "            'and mobile app.',\n",
            "  'question': 'Do you have live chat support?',\n",
            "  'similarity': 0.8336710929870605,\n",
            "  'source': 'Customer Support'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='Tell me order status for order id ORD-1234' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[route_intent] input='Tell me order status for order id ORD-1234' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, input='Tell me order status for order id ORD-1234'\n",
            "================================================================================\n",
            "🧑 User Query: Tell me order status for order id ORD-1234\n",
            "➡️ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "🤖 Agent Answer: Your order is being processed and will be shipped soon.\n",
            "🛠️ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='I want a return because the product is wrong' -> intent=human_escalation, dept=Orders & Returns, sentiment=negative\n",
            "[route_intent] input='I want a return because the product is wrong' -> intent=human_escalation, dept=Orders & Returns, sentiment=negative\n",
            "[tool_node] intent=human_escalation, dept=Orders & Returns, input='I want a return because the product is wrong'\n",
            "================================================================================\n",
            "🧑 User Query: I want a return because the product is wrong\n",
            "➡️ Intent: human_escalation | Dept: Orders & Returns | Sentiment: negative\n",
            "🤖 Agent Answer: I’m sorry for the inconvenience. Escalating to human support — someone will reach out to you soon.\n",
            "🛠️ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input=\"My password reset isn't working, this is frustrating\" -> intent=human_escalation, dept=HR & IT Helpdesk, sentiment=negative\n",
            "[route_intent] input=\"My password reset isn't working, this is frustrating\" -> intent=human_escalation, dept=HR & IT Helpdesk, sentiment=negative\n",
            "[tool_node] intent=human_escalation, dept=HR & IT Helpdesk, input=\"My password reset isn't working, this is frustrating\"\n",
            "================================================================================\n",
            "🧑 User Query: My password reset isn't working, this is frustrating\n",
            "➡️ Intent: human_escalation | Dept: HR & IT Helpdesk | Sentiment: negative\n",
            "🤖 Agent Answer: I’m sorry for the inconvenience. Escalating to human support — someone will reach out to you soon.\n",
            "🛠️ Tools Used: ['escalation']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='I submitted a complaint about a support issue' -> intent=ticket, dept=Customer Support, sentiment=neutral\n",
            "[route_intent] input='I submitted a complaint about a support issue' -> intent=ticket, dept=Customer Support, sentiment=neutral\n",
            "[tool_node] intent=ticket, dept=Customer Support, input='I submitted a complaint about a support issue'\n",
            "================================================================================\n",
            "🧑 User Query: I submitted a complaint about a support issue\n",
            "➡️ Intent: ticket | Dept: Customer Support | Sentiment: neutral\n",
            "🤖 Agent Answer: A support ticket has been created. Someone will get back to you shortly.\n",
            "🛠️ Tools Used: ['ticket_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='How do I pay with UPI?' -> intent=rag, dept=Payments & Billing, sentiment=neutral\n",
            "[route_intent] input='How do I pay with UPI?' -> intent=rag, dept=Payments & Billing, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=Payments & Billing, input='How do I pay with UPI?'\n",
            "[tool_node] Top cosine similarity=0.8775\n",
            "================================================================================\n",
            "🧑 User Query: How do I pay with UPI?\n",
            "➡️ Intent: rag | Dept: Payments & Billing | Sentiment: neutral\n",
            "🤖 Agent Answer: At checkout, select UPI, click 'Add New ID', and complete verification. (Dept: Payments & Billing)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': \"At checkout, select UPI, click 'Add New ID', and complete \"\n",
            "            'verification.',\n",
            "  'question': 'How do I add a new UPI ID?',\n",
            "  'similarity': 0.8774973750114441,\n",
            "  'source': 'Payments & Billing'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='How to apply for leaves?' -> intent=rag, dept=HR & IT Helpdesk, sentiment=neutral\n",
            "[route_intent] input='How to apply for leaves?' -> intent=rag, dept=HR & IT Helpdesk, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, input='How to apply for leaves?'\n",
            "[tool_node] Top cosine similarity=0.9056\n",
            "================================================================================\n",
            "🧑 User Query: How to apply for leaves?\n",
            "➡️ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "🤖 Agent Answer: You can apply for leave via the HRMS portal with manager approval. (Dept: HR & IT Helpdesk)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': 'You can apply for leave via the HRMS portal with manager '\n",
            "            'approval.',\n",
            "  'question': 'How to apply for leaves?',\n",
            "  'similarity': 0.9055670499801636,\n",
            "  'source': 'HR & IT Helpdesk'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='what is the leave policy?' -> intent=rag, dept=HR & IT Helpdesk, sentiment=neutral\n",
            "[route_intent] input='what is the leave policy?' -> intent=rag, dept=HR & IT Helpdesk, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, input='what is the leave policy?'\n",
            "[tool_node] Top cosine similarity=0.8766\n",
            "================================================================================\n",
            "🧑 User Query: what is the leave policy?\n",
            "➡️ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "🤖 Agent Answer: Employees are entitled to 20 days of annual leave, with manager approval required. (Dept: HR & IT Helpdesk)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': 'Employees are entitled to 20 days of annual leave, with manager '\n",
            "            'approval required.',\n",
            "  'question': 'What is the leave policy?',\n",
            "  'similarity': 0.8765844106674194,\n",
            "  'source': 'HR & IT Helpdesk'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='where is my order?' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[route_intent] input='where is my order?' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, input='where is my order?'\n",
            "================================================================================\n",
            "🧑 User Query: where is my order?\n",
            "➡️ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "🤖 Agent Answer: Your order is being processed and will be shipped soon.\n",
            "🛠️ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='my order is delayed' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[route_intent] input='my order is delayed' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, input='my order is delayed'\n",
            "================================================================================\n",
            "🧑 User Query: my order is delayed\n",
            "➡️ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "🤖 Agent Answer: Your order is being processed and will be shipped soon.\n",
            "🛠️ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='whats the return polcy? how many days can i return the product?' -> intent=rag, dept=Orders & Returns, sentiment=neutral\n",
            "[route_intent] input='whats the return polcy? how many days can i return the product?' -> intent=rag, dept=Orders & Returns, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=Orders & Returns, input='whats the return polcy? how many days can i return the product?'\n",
            "[tool_node] Top cosine similarity=0.8657\n",
            "================================================================================\n",
            "🧑 User Query: whats the return polcy? how many days can i return the product?\n",
            "➡️ Intent: rag | Dept: Orders & Returns | Sentiment: neutral\n",
            "🤖 Agent Answer: You can return items within 10 days of delivery for a full refund. (Dept: Orders & Returns)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': 'You can return items within 10 days of delivery for a full '\n",
            "            'refund.',\n",
            "  'question': 'What is the return policy?',\n",
            "  'similarity': 0.8657300472259521,\n",
            "  'source': 'Orders & Returns'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='how many leaves can I take?' -> intent=rag, dept=HR & IT Helpdesk, sentiment=neutral\n",
            "[route_intent] input='how many leaves can I take?' -> intent=rag, dept=HR & IT Helpdesk, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=HR & IT Helpdesk, input='how many leaves can I take?'\n",
            "[tool_node] Top cosine similarity=0.8662\n",
            "================================================================================\n",
            "🧑 User Query: how many leaves can I take?\n",
            "➡️ Intent: rag | Dept: HR & IT Helpdesk | Sentiment: neutral\n",
            "🤖 Agent Answer: Employees can take up to 20 days of annual leave per year. Carryover is subject to approval. (Dept: HR & IT Helpdesk)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': 'Employees can take up to 20 days of annual leave per year. '\n",
            "            'Carryover is subject to approval.',\n",
            "  'question': 'How many leaves can I take?',\n",
            "  'similarity': 0.8662147521972656,\n",
            "  'source': 'HR & IT Helpdesk'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input=' how many days for rturn?' -> intent=rag, dept=None, sentiment=neutral\n",
            "[route_intent] input=' how many days for rturn?' -> intent=rag, dept=None, sentiment=neutral\n",
            "[tool_node] intent=rag, dept=None, input=' how many days for rturn?'\n",
            "[tool_node] Top cosine similarity=0.8400\n",
            "================================================================================\n",
            "🧑 User Query:  how many days for rturn?\n",
            "➡️ Intent: rag | Dept: None | Sentiment: neutral\n",
            "🤖 Agent Answer: Returns are processed within 5–7 business days after pickup. (Dept: Orders & Returns)\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "📄 Retrieved Context:\n",
            "[{'answer': 'Returns are processed within 5–7 business days after pickup.',\n",
            "  'question': 'How many days for return?',\n",
            "  'similarity': 0.8399887084960938,\n",
            "  'source': 'Orders & Returns'}]\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='my retturn is delayed' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[route_intent] input='my retturn is delayed' -> intent=order_status, dept=Orders & Returns, sentiment=neutral\n",
            "[tool_node] intent=order_status, dept=Orders & Returns, input='my retturn is delayed'\n",
            "================================================================================\n",
            "🧑 User Query: my retturn is delayed\n",
            "➡️ Intent: order_status | Dept: Orders & Returns | Sentiment: neutral\n",
            "🤖 Agent Answer: Your order is being processed and will be shipped soon.\n",
            "🛠️ Tools Used: ['order_status_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='Please replace my shirt size' -> intent=return_create, dept=Orders & Returns, sentiment=positive\n",
            "[route_intent] input='Please replace my shirt size' -> intent=return_create, dept=Orders & Returns, sentiment=positive\n",
            "[tool_node] intent=return_create, dept=Orders & Returns, input='Please replace my shirt size'\n",
            "================================================================================\n",
            "🧑 User Query: Please replace my shirt size\n",
            "➡️ Intent: return_create | Dept: Orders & Returns | Sentiment: positive\n",
            "🤖 Agent Answer: Return initiated. You will receive pickup and label details via email.\n",
            "🛠️ Tools Used: ['return_create_tool']\n",
            "================================================================================\n",
            "\n",
            "[route_intent] input='I need help,' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "[route_intent] input='I need help,' -> intent=rag, dept=Customer Support, sentiment=positive\n",
            "[tool_node] intent=rag, dept=Customer Support, input='I need help,'\n",
            "[tool_node] Top cosine similarity=0.7491\n",
            "================================================================================\n",
            "🧑 User Query: I need help,\n",
            "➡️ Intent: rag | Dept: Customer Support | Sentiment: positive\n",
            "🤖 Agent Answer: 🤖 Sorry, I don’t know the exact answer to that. Escalating to human support.\n",
            "🛠️ Tools Used: ['rag_retrieval']\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell C - Flask API with ngrok (collision-safe, deep debug, auto-reuse, frontend-sync)\n",
        "# =========================\n",
        "import os, sys, traceback, threading, uuid, socket\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "# --------- Flask Setup (separate var from LangGraph 'app') ---------\n",
        "flask_app = Flask(__name__)\n",
        "CORS(flask_app)\n",
        "\n",
        "def _debug(msg: str):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# --------- Agent Caller ---------\n",
        "def call_agent(query: str) -> str:\n",
        "    \"\"\"Call either ask() or a compiled LangGraph app if available.\"\"\"\n",
        "    if \"ask\" in globals() and callable(globals()[\"ask\"]):\n",
        "        _debug(\"[AGENT] Using ask()\")\n",
        "        return globals()[\"ask\"](query)\n",
        "\n",
        "    for name in [\"agent_app\", \"graph_app\", \"app\"]:\n",
        "        obj = globals().get(name)\n",
        "        if hasattr(obj, \"invoke\"):\n",
        "            _debug(f\"[AGENT] Using graph '{name}'.invoke()\")\n",
        "            cfg = {\"configurable\": {\"thread_id\": f\"api-{uuid.uuid4().hex}\"}}\n",
        "            out = obj.invoke({\"user_input\": query}, config=cfg)\n",
        "            return out.get(\"answer\", \"No answer generated.\")\n",
        "\n",
        "    raise RuntimeError(\"❌ No agent available. Run Cell B first.\")\n",
        "\n",
        "# --------- Routes ---------\n",
        "@flask_app.route(\"/ask\", methods=[\"POST\", \"GET\"])\n",
        "def ask_api():\n",
        "    try:\n",
        "        _debug(\"\\n[API] ▶️ Received /ask\")\n",
        "        if request.method == \"POST\":\n",
        "            if not request.is_json:\n",
        "                return jsonify({\"error\": \"Content-Type must be application/json\"}), 400\n",
        "            data = request.get_json(force=True, silent=True) or {}\n",
        "            query = (data.get(\"query\") or \"\").strip()\n",
        "        else:\n",
        "            query = (request.args.get(\"query\") or \"\").strip()\n",
        "\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Empty query\"}), 400\n",
        "\n",
        "        _debug(f\"[API] Query: {query!r}\")\n",
        "        answer = call_agent(query)\n",
        "        _debug(f\"[API] ✅ Answer: {answer!r}\")\n",
        "        return jsonify({\"query\": query, \"answer\": answer})\n",
        "\n",
        "    except Exception as e:\n",
        "        _debug(\"[API] ❌ Internal server error\")\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "        return jsonify({\"error\": \"Internal server error\", \"details\": str(e)}), 500\n",
        "\n",
        "@flask_app.route(\"/\", methods=[\"GET\"])\n",
        "def home():\n",
        "    return jsonify({\"status\": \"ok\", \"message\": \"ShopUNow Agent API is running!\"})\n",
        "\n",
        "# --------- Run Flask (dynamic port, avoids collisions) ---------\n",
        "def find_free_port(default=5000):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind((\"\", 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "PORT = find_free_port(5000)\n",
        "_debug(f\"▶️ Starting Flask server on port {PORT}...\")\n",
        "\n",
        "def run_flask():\n",
        "    try:\n",
        "        flask_app.run(host=\"0.0.0.0\", port=PORT, debug=False, use_reloader=False)\n",
        "    except Exception as e:\n",
        "        _debug(f\"❌ Flask crashed: {e}\")\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "threading.Thread(target=run_flask, daemon=True).start()\n",
        "\n",
        "# --------- ngrok setup ---------\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "except ImportError:\n",
        "    _debug(\"[NGROK] Installing pyngrok...\")\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyngrok\"], check=True)\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "def _get_secret(name):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(name)\n",
        "    except Exception:\n",
        "        return os.getenv(name)\n",
        "\n",
        "NGROK_AUTH_TOKEN = _get_secret(\"NGROK_AUTH_TOKEN\")\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    _debug(\"🔑 Setting ngrok auth token\")\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "else:\n",
        "    _debug(\"⚠️ No ngrok auth token found; using free session\")\n",
        "\n",
        "# Kill old tunnels before opening a new one\n",
        "ngrok.kill()\n",
        "\n",
        "try:\n",
        "    _debug(f\"🌐 Starting ngrok tunnel on :{PORT} ...\")\n",
        "    tunnel = ngrok.connect(PORT)\n",
        "    public_url = getattr(tunnel, \"public_url\", str(tunnel))\n",
        "    _debug(f\"🚀 Public API URL: {public_url}\")\n",
        "\n",
        "    # ---- Write URL for frontend auto-detection ----\n",
        "    backend_file = \"/content/backend_url.txt\"\n",
        "    with open(backend_file, \"w\") as f:\n",
        "        f.write(public_url.strip())\n",
        "    _debug(f\"📂 Saved backend URL to {backend_file}\")\n",
        "\n",
        "    # Print curl test\n",
        "    print(\"\\nTest with:\")\n",
        "    print(f'curl -X POST \"{public_url}/ask\" -H \"Content-Type: application/json\" -d \"{{\\\\\"query\\\\\": \\\\\"What are your support hours?\\\\\"}}\"')\n",
        "except Exception as e:\n",
        "    _debug(f\"❌ ngrok connection failed: {e}\")\n",
        "    traceback.print_exc(file=sys.stdout)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgBZx70SRRCQ",
        "outputId": "0a564a2c-7b44-48ec-fa30-1e109207cd74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▶️ Starting Flask server on port 50067...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:50067\n",
            " * Running on http://172.28.0.12:50067\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Setting ngrok auth token\n",
            "🌐 Starting ngrok tunnel on :50067 ...\n",
            "🚀 Public API URL: https://c0792a802d11.ngrok-free.app\n",
            "📂 Saved backend URL to /content/backend_url.txt\n",
            "\n",
            "Test with:\n",
            "curl -X POST \"https://c0792a802d11.ngrok-free.app/ask\" -H \"Content-Type: application/json\" -d \"{\\\"query\\\": \\\"What are your support hours?\\\"}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell D - Streamlit Frontend (Auto-detect backend, Mandatory Keys, LangSmith optional)\n",
        "# =========================\n",
        "import subprocess, threading, os\n",
        "\n",
        "!pip install -q streamlit requests pyngrok\n",
        "\n",
        "# Try to auto-read backend URL from file\n",
        "backend_url_file = \"/content/backend_url.txt\"\n",
        "default_api_url = \"http://127.0.0.1:5000/ask\"\n",
        "if os.path.exists(backend_url_file):\n",
        "    try:\n",
        "        with open(backend_url_file, \"r\") as f:\n",
        "            url = f.read().strip()\n",
        "            if url:\n",
        "                default_api_url = url.rstrip(\"/\") + \"/ask\"\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Could not read backend_url.txt:\", e)\n",
        "\n",
        "with open(\"app_frontend.py\", \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "st.set_page_config(page_title=\"ShopUNow Agent\", page_icon=\"🛍️\", layout=\"centered\")\n",
        "st.title(\"🛍️ ShopUNow AI Assistant\")\n",
        "\n",
        "# --- Sidebar: Configuration ---\n",
        "st.sidebar.header(\"🔑 Configuration (Required)\")\n",
        "\n",
        "api_url = st.sidebar.text_input(\"Flask API URL (auto-detected from backend)\", value=\"{default_api_url}\")\n",
        "openai_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "gemini_key = st.sidebar.text_input(\"Gemini API Key\", type=\"password\")\n",
        "ngrok_token = st.sidebar.text_input(\"ngrok Auth Token\", type=\"password\")\n",
        "\n",
        "st.sidebar.header(\"Optional\")\n",
        "langsmith_key = st.sidebar.text_input(\"LangSmith Key (optional)\", type=\"password\")\n",
        "\n",
        "# ---- Validation ----\n",
        "errors = []\n",
        "if not api_url.strip():\n",
        "    errors.append(\"❌ Flask API URL is required.\")\n",
        "if not (openai_key.strip() or gemini_key.strip()):\n",
        "    errors.append(\"❌ At least one model key (OpenAI or Gemini) is required.\")\n",
        "if not ngrok_token.strip():\n",
        "    errors.append(\"❌ ngrok Auth Token is required.\")\n",
        "\n",
        "if errors:\n",
        "    for e in errors:\n",
        "        st.sidebar.error(e)\n",
        "    st.stop()\n",
        "else:\n",
        "    st.sidebar.success(\"✅ All required configuration set\")\n",
        "\n",
        "# Store secrets\n",
        "st.session_state.secrets = {{\n",
        "    \"API_URL\": api_url.strip(),\n",
        "    \"OPENAI_API_KEY\": openai_key.strip(),\n",
        "    \"GEMINI_API_KEY\": gemini_key.strip(),\n",
        "    \"NGROK_AUTH_TOKEN\": ngrok_token.strip(),\n",
        "    \"LANGSMITH_KEY\": langsmith_key.strip(),\n",
        "}}\n",
        "\n",
        "# --- Chat Section ---\n",
        "st.subheader(\"💬 Chat with ShopUNow Agent\")\n",
        "\n",
        "if \"chat\" not in st.session_state:\n",
        "    st.session_state.chat = []\n",
        "\n",
        "query = st.text_input(\"Type your question here:\")\n",
        "\n",
        "if st.button(\"Ask\"):\n",
        "    if query.strip():\n",
        "        st.session_state.chat.append((\"🧑 You\", query))\n",
        "        try:\n",
        "            resp = requests.post(st.session_state.secrets[\"API_URL\"], json={{\"query\": query}}, timeout=20)\n",
        "            if resp.status_code == 200:\n",
        "                data = resp.json()\n",
        "                answer = data.get(\"answer\", \"⚠️ No answer returned\")\n",
        "            else:\n",
        "                answer = f\"⚠️ Error {{resp.status_code}}: {{resp.text}}\"\n",
        "        except Exception as e:\n",
        "            answer = f\"⚠️ Request failed: {{e}}\"\n",
        "\n",
        "        st.session_state.chat.append((\"🤖 Agent\", answer))\n",
        "\n",
        "# --- Display chat history ---\n",
        "for sender, msg in st.session_state.chat:\n",
        "    st.markdown(f\"**{{sender}}:** {{msg}}\")\n",
        "\"\"\")\n",
        "\n",
        "# ---- Run Streamlit in background ----\n",
        "def run_streamlit():\n",
        "    subprocess.run(\n",
        "        [\"streamlit\", \"run\", \"app_frontend.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"]\n",
        "    )\n",
        "\n",
        "threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "\n",
        "# ---- ngrok tunnel for frontend ----\n",
        "from pyngrok import ngrok\n",
        "print(\"🌐 Starting ngrok tunnel for Streamlit...\")\n",
        "frontend_url = ngrok.connect(8501)\n",
        "print(\"🚀 Streamlit Frontend URL:\", frontend_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgocOgNHW72d",
        "outputId": "9fb12362-3164-454b-b5b7-65797fbe3120"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Starting ngrok tunnel for Streamlit...\n",
            "🚀 Streamlit Frontend URL: NgrokTunnel: \"https://33384fb0fead.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}